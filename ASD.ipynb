{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aec54eb-a0c9-451d-8448-e953a9bb551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahjc\\anaconda3\\envs\\llava\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% 0) Imports\n",
    "import os, re, json, csv, pathlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Iterable\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ee2c6f-3ccd-4e7a-8534-1fd633ff00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 16:41:31) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch: 2.9.0+cu126\n",
      "Transformers: 4.57.1\n"
     ]
    }
   ],
   "source": [
    "# %% A) Make sure the right packages/classes are available\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "try:\n",
    "    import torch, transformers\n",
    "    from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"Transformers:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    print(\"Import check failed:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd8f663-aec6-43d4-9a00-db0e995da0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "MODEL: llava-hf/llava-1.5-7b-hf\n",
      "POPE_ROOT: POPE/output/coco\n",
      "COCO_IMG_ROOT: val2014\n"
     ]
    }
   ],
   "source": [
    "# %% 1) Config — EDIT THESE\n",
    "POPE_ROOT      = \"POPE/output/coco\"   # folder with random/popular/adversarial .json/.jsonl\n",
    "COCO_IMG_ROOT  = \"val2014\"     # folder with COCO_val2014_*.jpg\n",
    "HF_MODEL_ID    = \"llava-hf/llava-1.5-7b-hf\"      # e.g., \"llava-hf/llava-1.5-7b-hf\" or \"llava-hf/llava-1.6-mistral-7b-hf\"\n",
    "\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE          = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "MAX_NEW_TOKENS = 8\n",
    "TEMPERATURE    = 0.0\n",
    "CSV_OUT        = \"pope_coco_predictions.csv\"\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"MODEL:\", HF_MODEL_ID)\n",
    "print(\"POPE_ROOT:\", POPE_ROOT)\n",
    "print(\"COCO_IMG_ROOT:\", COCO_IMG_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6294fb6a-bf1b-49c4-9996-d07ddfbd75bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 JSON/JSONL files under POPE\\output\\coco:\n",
      " - coco_pope_adversarial.json\n",
      " - coco_pope_popular.json\n",
      " - coco_pope_random.json\n",
      " - .ipynb_checkpoints\\coco_pope_random-checkpoint.json\n"
     ]
    }
   ],
   "source": [
    "# List json/jsonl files under your POPE_ROOT so we can see real names/paths\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(POPE_ROOT)\n",
    "assert root.exists(), f\"POPE_ROOT not found: {root}\"\n",
    "\n",
    "cands = list(root.rglob(\"*.json\")) + list(root.rglob(\"*.jsonl\"))\n",
    "print(f\"Found {len(cands)} JSON/JSONL files under {root}:\")\n",
    "for p in cands[:50]:\n",
    "    print(\" -\", p.relative_to(root))\n",
    "if len(cands) == 0:\n",
    "    print(\"No JSON files found. Double-check POPE_ROOT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "603c56e5-af6d-4228-ab72-b281b9c1444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2) POPE loader (robust to .json vs .jsonl) + image id normalizer\n",
    "# %% FIXED: auto-detect loader for POPE files (handles underscores, avoids -checkpoint)\n",
    "import json, pathlib\n",
    "\n",
    "def _iter_json_or_jsonl(path: pathlib.Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        txt = f.read().strip()\n",
    "    # Try single JSON doc\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        if isinstance(obj, dict) and \"data\" in obj:\n",
    "            for row in obj[\"data\"]:\n",
    "                yield row\n",
    "            return\n",
    "        if isinstance(obj, list):\n",
    "            for row in obj:\n",
    "                yield row\n",
    "            return\n",
    "        if isinstance(obj, dict):\n",
    "            yield obj\n",
    "            return\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # Fallback: JSONL\n",
    "    for line in txt.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def _to_val2014_filename(x):\n",
    "    if isinstance(x, int):\n",
    "        return f\"COCO_val2014_{x:012d}.jpg\"\n",
    "    if isinstance(x, str):\n",
    "        return os.path.basename(x)\n",
    "    raise TypeError(f\"Unsupported image ref type: {type(x)}\")\n",
    "\n",
    "def load_pope_split_autodetect(pope_root: str) -> dict:\n",
    "    \"\"\"\n",
    "    Recursively scans for files whose names contain 'random' / 'popular' / 'adversarial'\n",
    "    (case-insensitive), prefers non '-checkpoint' files, picks the largest if multiples.\n",
    "    Accepts .json OR .jsonl; accepts {'data': [...]} or JSONL lines.\n",
    "    Normalizes rows to {image, question, answer in {'yes','no'}}.\n",
    "    \"\"\"\n",
    "    root = pathlib.Path(pope_root)\n",
    "    assert root.exists(), f\"POPE_ROOT not found: {root}\"\n",
    "\n",
    "    files = list(root.rglob(\"*.json\")) + list(root.rglob(\"*.jsonl\"))\n",
    "    assert files, f\"No .json/.jsonl found under {root}\"\n",
    "\n",
    "    def pick_for(key: str) -> pathlib.Path:\n",
    "        key_l = key.lower()\n",
    "        cands = [p for p in files if key_l in p.name.lower()]\n",
    "        # Prefer non-checkpoint\n",
    "        primary = [p for p in cands if \"checkpoint\" not in p.name.lower()]\n",
    "        use = primary or cands\n",
    "        if not use:\n",
    "            raise FileNotFoundError(f\"No file name containing '{key}' under {root}. Found: {[p.name for p in files][:10]}\")\n",
    "        # choose the largest (often the real data file)\n",
    "        use.sort(key=lambda p: p.stat().st_size, reverse=True)\n",
    "        return use[0]\n",
    "\n",
    "    selected = {\n",
    "        \"random\": pick_for(\"random\"),\n",
    "        \"popular\": pick_for(\"popular\"),\n",
    "        \"adversarial\": pick_for(\"adversarial\"),\n",
    "    }\n",
    "\n",
    "    print(\"Selected files (auto):\")\n",
    "    for k, p in selected.items():\n",
    "        print(f\" - {k}: {p.relative_to(root)}\")\n",
    "\n",
    "    out = {}\n",
    "    for split, path in selected.items():\n",
    "        rows = []\n",
    "        for r in _iter_json_or_jsonl(path):\n",
    "            ans = str(r.get(\"answer\", \"\")).strip().lower()\n",
    "            if ans in (\"1\",\"true\",\"yes\",\"y\"): ans = \"yes\"\n",
    "            elif ans in (\"0\",\"false\",\"no\",\"n\"): ans = \"no\"\n",
    "            img = r.get(\"image\") or r.get(\"image_id\") or r.get(\"img\") or r.get(\"img_path\")\n",
    "            q   = r.get(\"question\") or r.get(\"q\") or r.get(\"text\")\n",
    "            rows.append({\n",
    "                \"image\": _to_val2014_filename(img) if img is not None else None,\n",
    "                \"question\": q,\n",
    "                \"answer\": ans\n",
    "            })\n",
    "        rows = [x for x in rows if x[\"image\"] and x[\"question\"] and x[\"answer\"] in {\"yes\",\"no\"}]\n",
    "        if not rows:\n",
    "            raise RuntimeError(f\"Loaded 0 normalized rows for split '{split}' from {path}\")\n",
    "        out[split] = rows\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62459384-2915-4016-acc9-319c2fd90d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 POPE json/jsonl under POPE\\output\\coco:\n",
      " - coco_pope_adversarial.json\n",
      " - coco_pope_popular.json\n",
      " - coco_pope_random.json\n",
      " - .ipynb_checkpoints\\coco_pope_random-checkpoint.json\n",
      "\n",
      "=== Peek: POPE\\output\\coco\\coco_pope_random.json ===\n",
      "Total items: 3000\n",
      "\n",
      "-- Row 0 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a snowboard in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "-- Row 1 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a car in the image?'\n",
      "  label => 'no'\n",
      "\n",
      "-- Row 2 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a person in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "-- Row 3 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a sandwich in the image?'\n",
      "  label => 'no'\n",
      "\n",
      "-- Row 4 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a skis in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "=== Peek: POPE\\output\\coco\\coco_pope_popular.json ===\n",
      "Total items: 3000\n",
      "\n",
      "-- Row 0 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a snowboard in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "-- Row 1 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a dining table in the image?'\n",
      "  label => 'no'\n",
      "\n",
      "-- Row 2 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a person in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "-- Row 3 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a car in the image?'\n",
      "  label => 'no'\n",
      "\n",
      "-- Row 4 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a skis in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "=== Peek: POPE\\output\\coco\\coco_pope_adversarial.json ===\n",
      "Total items: 3000\n",
      "\n",
      "-- Row 0 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a snowboard in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "-- Row 1 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a backpack in the image?'\n",
      "  label => 'no'\n",
      "\n",
      "-- Row 2 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a person in the image?'\n",
      "  label => 'yes'\n",
      "\n",
      "-- Row 3 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a car in the image?'\n",
      "  label => 'no'\n",
      "\n",
      "-- Row 4 keys: ['question_id', 'image', 'text', 'label']\n",
      "  image => 'COCO_val2014_000000310196.jpg'\n",
      "  text => 'Is there a skis in the image?'\n",
      "  label => 'yes'\n"
     ]
    }
   ],
   "source": [
    "# Inspect what's actually inside your POPE files\n",
    "from pathlib import Path\n",
    "import json, itertools\n",
    "\n",
    "def peek_pope_file(path, n=5):\n",
    "    p = Path(path)\n",
    "    print(f\"\\n=== Peek: {p} ===\")\n",
    "    txt = p.read_text(encoding=\"utf-8\").strip()\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "    except json.JSONDecodeError:\n",
    "        # JSONL fallback\n",
    "        lines = [json.loads(ln) for ln in txt.splitlines() if ln.strip()]\n",
    "        obj = lines\n",
    "\n",
    "    rows = obj.get(\"data\", obj) if isinstance(obj, dict) else obj\n",
    "    if not isinstance(rows, list):\n",
    "        print(\"Unexpected top-level type:\", type(rows))\n",
    "        return\n",
    "\n",
    "    print(\"Total items:\", len(rows))\n",
    "    for i, r in enumerate(itertools.islice(rows, n)):\n",
    "        print(f\"\\n-- Row {i} keys:\", list(r.keys()))\n",
    "        # show a compact preview of key fields if present\n",
    "        for k in [\"image\",\"image_id\",\"img\",\"img_id\",\"img_path\",\"file_name\",\n",
    "                  \"question\",\"q\",\"text\",\"prompt\",\n",
    "                  \"answer\",\"label\",\"gt\",\"target\",\"ans\",\"response\",\"y\"]:\n",
    "            if k in r:\n",
    "                print(f\"  {k} => {r[k]!r}\")\n",
    "\n",
    "# List JSON files and peek the three we’ll use\n",
    "from pathlib import Path\n",
    "root = Path(POPE_ROOT)\n",
    "cands = list(root.rglob(\"*.json\")) + list(root.rglob(\"*.jsonl\"))\n",
    "print(f\"Found {len(cands)} POPE json/jsonl under {root}:\")\n",
    "for p in cands:\n",
    "    print(\" -\", p.relative_to(root))\n",
    "\n",
    "# If your files are named like coco_pope_*.json, peek them directly:\n",
    "peek_pope_file(root / \"coco_pope_random.json\")\n",
    "peek_pope_file(root / \"coco_pope_popular.json\")\n",
    "peek_pope_file(root / \"coco_pope_adversarial.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec006b73-1f81-43d6-afe2-73af08e014c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust POPE loader: tolerates many schema variants\n",
    "import json, pathlib, os\n",
    "\n",
    "YES_TOKENS = {\"1\",\"true\",\"yes\",\"y\",\"present\",\"a\"}   # include 'a' as some datasets use A/B\n",
    "NO_TOKENS  = {\"0\",\"false\",\"no\",\"n\",\"absent\",\"b\"}\n",
    "\n",
    "def _read_any(path: pathlib.Path):\n",
    "    txt = path.read_text(encoding=\"utf-8\").strip()\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        if isinstance(obj, dict) and \"data\" in obj:\n",
    "            return obj[\"data\"]\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "        if isinstance(obj, dict):\n",
    "            return [obj]\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # JSONL fallback\n",
    "    return [json.loads(ln) for ln in txt.splitlines() if ln.strip()]\n",
    "\n",
    "def _canon_answer(row):\n",
    "    # try common fields\n",
    "    raw = None\n",
    "    for k in (\"answer\",\"label\",\"gt\",\"target\",\"ans\",\"response\",\"y\"):\n",
    "        if k in row:\n",
    "            raw = row[k]\n",
    "            break\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # normalize types\n",
    "    if isinstance(raw, bool):\n",
    "        return \"yes\" if raw else \"no\"\n",
    "    if isinstance(raw, (int, float)):\n",
    "        return \"yes\" if int(raw) == 1 else \"no\"\n",
    "    s = str(raw).strip().lower()\n",
    "    if s in YES_TOKENS: return \"yes\"\n",
    "    if s in NO_TOKENS:  return \"no\"\n",
    "    # sometimes options are like {\"A\":\"yes\",\"B\":\"no\"} and label is \"A\"/\"B\"\n",
    "    if s in {\"a\",\"b\"}:\n",
    "        # heuristic: map A->yes, B->no unless 'options' says otherwise\n",
    "        opts = row.get(\"options\") or row.get(\"choices\")\n",
    "        if isinstance(opts, dict):\n",
    "            inv = {v.strip().lower(): k.lower() for k,v in opts.items()}\n",
    "            # if options say 'yes' maps to 'a' explicitly, honor that\n",
    "            if \"yes\" in inv and \"no\" in inv:\n",
    "                return \"yes\" if s == inv[\"yes\"] else \"no\"\n",
    "        return \"yes\" if s == \"a\" else \"no\"\n",
    "    return None\n",
    "\n",
    "def _to_val2014_filename(x):\n",
    "    if isinstance(x, int):\n",
    "        return f\"COCO_val2014_{x:012d}.jpg\"\n",
    "    if isinstance(x, str):\n",
    "        return os.path.basename(x)\n",
    "    return None\n",
    "\n",
    "def _canon_image(row):\n",
    "    for k in (\"image\",\"image_id\",\"img\",\"img_id\",\"img_path\",\"file_name\",\"filename\",\"path\"):\n",
    "        if k in row:\n",
    "            v = row[k]\n",
    "            fn = _to_val2014_filename(v)\n",
    "            if fn:\n",
    "                return fn\n",
    "    return None\n",
    "\n",
    "def _canon_question(row):\n",
    "    for k in (\"question\",\"q\",\"text\",\"prompt\",\"instruction\"):\n",
    "        if k in row:\n",
    "            v = row[k]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip()\n",
    "    return None\n",
    "\n",
    "def _pick(pope_root: pathlib.Path, key_substr: str):\n",
    "    cands = [p for p in pope_root.rglob(\"*.json\")] + [p for p in pope_root.rglob(\"*.jsonl\")]\n",
    "    cands = [p for p in cands if key_substr.lower() in p.name.lower() and \"checkpoint\" not in p.name.lower()]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No file containing '{key_substr}' under {pope_root}\")\n",
    "    cands.sort(key=lambda p: p.stat().st_size, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def load_pope_split_robust(pope_root: str) -> dict:\n",
    "    root = pathlib.Path(pope_root)\n",
    "    assert root.exists(), f\"POPE_ROOT not found: {root}\"\n",
    "\n",
    "    files = {\n",
    "        \"random\": _pick(root, \"random\"),\n",
    "        \"popular\": _pick(root, \"popular\"),\n",
    "        \"adversarial\": _pick(root, \"adversarial\"),\n",
    "    }\n",
    "    print(\"Using files:\")\n",
    "    for k,p in files.items():\n",
    "        print(f\" - {k}: {p.relative_to(root)} (size {p.stat().st_size/1024:.1f} KB)\")\n",
    "\n",
    "    out = {}\n",
    "    for split, path in files.items():\n",
    "        raw_rows = _read_any(path)\n",
    "        norm = []\n",
    "        for r in raw_rows:\n",
    "            img = _canon_image(r)\n",
    "            q   = _canon_question(r)\n",
    "            a   = _canon_answer(r)\n",
    "            if img and q and a in {\"yes\",\"no\"}:\n",
    "                norm.append({\"image\": img, \"question\": q, \"answer\": a})\n",
    "        if not norm:\n",
    "            # show a few raw rows to help debug\n",
    "            print(f\"\\n[WARN] 0 normalized rows for {split}. First raw rows:\")\n",
    "            for sample in raw_rows[:3]:\n",
    "                print(sample)\n",
    "            raise RuntimeError(f\"0 normalized rows for split '{split}' from {path}\")\n",
    "        out[split] = norm\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053477f0-9b4a-43a6-914d-b1e0715338e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files:\n",
      " - random: coco_pope_random.json (size 362.5 KB)\n",
      " - popular: coco_pope_popular.json (size 361.5 KB)\n",
      " - adversarial: coco_pope_adversarial.json (size 361.7 KB)\n",
      "random: 3000 rows\n",
      "  sample: {'image': 'COCO_val2014_000000310196.jpg', 'question': 'Is there a snowboard in the image?', 'answer': 'yes'}\n",
      "popular: 3000 rows\n",
      "  sample: {'image': 'COCO_val2014_000000310196.jpg', 'question': 'Is there a snowboard in the image?', 'answer': 'yes'}\n",
      "adversarial: 3000 rows\n",
      "  sample: {'image': 'COCO_val2014_000000310196.jpg', 'question': 'Is there a snowboard in the image?', 'answer': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "pope = load_pope_split_robust(POPE_ROOT)\n",
    "\n",
    "for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "    print(f\"{split}: {len(pope[split])} rows\")\n",
    "    print(\"  sample:\", pope[split][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c2435b-0790-45d3-868b-dc939c368535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "oading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:08<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF LLaVA: llava-hf/llava-1.5-7b-hf on cuda\n"
     ]
    }
   ],
   "source": [
    "# %% 4) Load LLaVA from Hugging Face\n",
    "processor = AutoProcessor.from_pretrained(HF_MODEL_ID)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Loaded HF LLaVA:\", HF_MODEL_ID, \"on\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d39e92-4f79-4fce-8ca0-fda96bfc06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 5) Hugging Face LLaVA inference (replaces llava_answer_yesno/infer_yesno)\n",
    "\n",
    "YES_RE = re.compile(r\"\\byes\\b\", re.I)\n",
    "NO_RE  = re.compile(r\"\\bno\\b\", re.I)\n",
    "\n",
    "def normalize_yesno(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"no\"\n",
    "    t = text.strip().lower()\n",
    "    ys = [m.start() for m in YES_RE.finditer(t)]\n",
    "    ns = [m.start() for m in NO_RE.finditer(t)]\n",
    "    if ys or ns:\n",
    "        return \"yes\" if (ys[-1] if ys else -1) > (ns[-1] if ns else -1) else \"no\"\n",
    "    return \"yes\" if t.startswith(\"y\") else \"no\"\n",
    "\n",
    "\n",
    "def hf_llava_answer_yesno(processor, model, image_path: str, question: str,\n",
    "                          max_new_tokens=8, temperature=0.0) -> str:\n",
    "    \"\"\"\n",
    "    Run HuggingFace LlavaForConditionalGeneration on one (image, question) pair.\n",
    "    Returns 'yes' or 'no'.\n",
    "    \"\"\"\n",
    "    # prompt format for HF Llava models\n",
    "    prompt = (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. \"\n",
    "        \"Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"ASSISTANT:\"\n",
    "    )\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False if temperature == 0.0 else True,\n",
    "            temperature=float(temperature),\n",
    "            max_new_tokens=int(max_new_tokens),\n",
    "        )\n",
    "\n",
    "    out = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return normalize_yesno(out)\n",
    "\n",
    "\n",
    "def infer_yesno(img_path: str, question: str) -> str:\n",
    "    return hf_llava_answer_yesno(\n",
    "        processor, model,\n",
    "        img_path, question,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f997022-6f56-47c0-8576-ebfe3cb54d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 6) Metrics + evaluator\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    tp:int=0; tn:int=0; fp:int=0; fn:int=0\n",
    "    def scores(self):\n",
    "        acc = (self.tp + self.tn) / max(1, self.tp + self.tn + self.fp + self.fn)\n",
    "        prec = self.tp / max(1, self.tp + self.fp)\n",
    "        rec  = self.tp / max(1, self.tp + self.fn)\n",
    "        f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "        return {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}\n",
    "\n",
    "def eval_rows(rows: List[dict], coco_img_root: str, infer_fn) -> dict:\n",
    "    m = Metrics()\n",
    "    for r in tqdm(rows, leave=False, desc=\"Evaluating\"):\n",
    "        img_file = r[\"image\"]\n",
    "        img_path = img_file if os.path.isabs(img_file) else os.path.join(coco_img_root, img_file)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue  # skip missing files\n",
    "        pred = infer_fn(img_path, r[\"question\"])\n",
    "        gt   = r[\"answer\"]\n",
    "        if   pred==\"yes\" and gt==\"yes\": m.tp += 1\n",
    "        elif pred==\"no\"  and gt==\"no\" : m.tn += 1\n",
    "        elif pred==\"yes\" and gt==\"no\" : m.fp += 1\n",
    "        elif pred==\"no\"  and gt==\"yes\": m.fn += 1\n",
    "    return m.scores()\n",
    "\n",
    "def pct(d):  # pretty-print as percentages\n",
    "    return {k: round(v*100, 2) for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f319e02c-53b0-4c7e-be13-87cc4b3c9025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running smoke test on 8 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOKE: {'Accuracy': 100.0, 'Precision': 100.0, 'Recall': 100.0, 'F1': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% 7) Smoke test\n",
    "N = 8\n",
    "if N:\n",
    "    test_rows = pope[\"random\"][:N]\n",
    "    print(\"Running smoke test on\", len(test_rows), \"items...\")\n",
    "    sm = eval_rows(test_rows, COCO_IMG_ROOT, infer_yesno)\n",
    "    print(\"SMOKE:\", pct(sm))\n",
    "else:\n",
    "    print(\"Skipping smoke test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebbf29a4-4fed-44dc-ab5b-ac4b8a512b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 88.83, 'Precision': 96.05, 'Recall': 81.0, 'F1': 87.88}\n",
      "- Popular     : {'Accuracy': 86.93, 'Precision': 91.91, 'Recall': 81.0, 'F1': 86.11}\n",
      "- Adversarial : {'Accuracy': 84.03, 'Precision': 86.23, 'Recall': 81.0, 'F1': 83.53}\n",
      "- Overall     : {'Accuracy': 86.6, 'Precision': 91.22, 'Recall': 81.0, 'F1': 85.81}\n",
      "\n",
      "Total time: 2889.7s for 9000 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% 8) Full POPE (COCO) run + report\n",
    "import time\n",
    "\n",
    "def run_full_pope(pope, coco_img_root, infer_fn):\n",
    "    results = {}\n",
    "    counts  = {}\n",
    "    all_rows = []\n",
    "    t0 = time.time()\n",
    "    for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"\\nRunning split: {split}\")\n",
    "        s = eval_rows(pope[split], coco_img_root, infer_fn)\n",
    "        results[split] = s\n",
    "        all_rows.extend(pope[split])\n",
    "    overall = eval_rows(all_rows, coco_img_root, infer_fn)\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(\"\\nPOPE (COCO) results:\")\n",
    "    for k in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"- {k.title():12}:\", pct(results[k]))\n",
    "    print(f\"- Overall     :\", pct(overall))\n",
    "    print(f\"\\nTotal time: {t1 - t0:.1f}s for {sum(len(pope[s]) for s in ('random','popular','adversarial'))} items\")\n",
    "    return results, overall\n",
    "\n",
    "results, overall = run_full_pope(pope, COCO_IMG_ROOT, infer_yesno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf3e1d89-cc86-448a-8fb7-3a25b73b3d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering vector shape: (4096,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% 11) Build a steering vector (activation contrast: image vs. image-blind)\n",
    "# Uses your already-loaded: processor, model, DEVICE; and your loaded 'pope' splits\n",
    "# Idea: for each (image, question), compare hidden states with the real image vs a blank image.\n",
    "# Steering dir v = mean( h_with_image - h_blank_image ) over a calibration set.\n",
    "\n",
    "import torch\n",
    "from PIL import Image, ImageColor\n",
    "\n",
    "@torch.no_grad()\n",
    "def _last_hidden_state(processor, model, image, question, return_full_output=False):\n",
    "    prompt = (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. \"\n",
    "        \"Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"ASSISTANT:\"\n",
    "    )\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    # out.hidden_states is the final LM hidden states (one per layer if enabled by the HF port).\n",
    "    # Some HF LLaVA ports expose only the final layer via `out.hidden_states[-1]`.\n",
    "    last_h = out.hidden_states[-1][:, -1, :]  # (batch=1, seq, dim) -> (1, dim) for last token\n",
    "    return (last_h, out) if return_full_output else last_h\n",
    "\n",
    "# A simple neutral \"blank\" image (224x224 mid-gray)\n",
    "_blank = Image.new(\"RGB\", (224, 224), ImageColor.getrgb(\"#808080\"))\n",
    "\n",
    "def build_steering_vector(pope, coco_img_root, processor, model, \n",
    "                          split=\"random\", \n",
    "                          num_calib=400, \n",
    "                          seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    rows = pope[split][:num_calib]\n",
    "    vecs = []\n",
    "    for r in tqdm(rows, desc=\"Calibrating (build v)\", leave=False):\n",
    "        img_path = os.path.join(coco_img_root, r[\"image\"])\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        h_real = _last_hidden_state(processor, model, img, r[\"question\"])   # (1, d)\n",
    "        h_blank= _last_hidden_state(processor, model, _blank, r[\"question\"])# (1, d)\n",
    "        vecs.append((h_real - h_blank).cpu())\n",
    "    if not vecs:\n",
    "        raise RuntimeError(\"No calibration vectors collected (check image paths).\")\n",
    "    v = torch.stack(vecs, dim=0).mean(dim=0).squeeze(0)  # (d,)\n",
    "    # Normalize to unit norm (helps tune alpha meaningfully)\n",
    "    v = v / (v.norm(p=2) + 1e-8)\n",
    "    return v\n",
    "\n",
    "steer_v = build_steering_vector(pope, COCO_IMG_ROOT, processor, model, split=\"random\", num_calib=400, seed=0)\n",
    "print(\"Steering vector shape:\", tuple(steer_v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9af117-dba9-4b49-b7c6-db8226341e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES_ID: 3869 NO_ID: 1939\n",
      "Steered smoke (alpha=1.0):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOKE (steered): {'Accuracy': 100.0, 'Precision': 100.0, 'Recall': 100.0, 'F1': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% 12) Steered yes/no inference\n",
    "# We apply steering at the final hidden state: h* = h + alpha * v\n",
    "# Then compute logits = lm_head(h*), compare Yes/No token scores.\n",
    "\n",
    "# Helper: get token ids for 'Yes' and 'No' continuations (LLaMA uses leading space)\n",
    "tok = processor.tokenizer\n",
    "YES_IDS = tok.encode(\" Yes\", add_special_tokens=False)\n",
    "NO_IDS  = tok.encode(\" No\",  add_special_tokens=False)\n",
    "YES_ID  = YES_IDS[-1] if YES_IDS else tok.encode(\"Yes\", add_special_tokens=False)[-1]\n",
    "NO_ID   = NO_IDS[-1]  if NO_IDS  else tok.encode(\"No\",  add_special_tokens=False)[-1]\n",
    "print(\"YES_ID:\", YES_ID, \"NO_ID:\", NO_ID)\n",
    "\n",
    "@torch.no_grad()\n",
    "def hf_llava_answer_yesno_steered(processor, model, image_path: str, question: str, \n",
    "                                  v: torch.Tensor, alpha: float = 1.0) -> str:\n",
    "    # Forward once to get hidden state for the last token (pre-logits)\n",
    "    prompt = (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. \"\n",
    "        \"Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"ASSISTANT:\"\n",
    "    )\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    # Grab hidden state for last position\n",
    "    h_last = out.hidden_states[-1][:, -1, :]              # (1, d)\n",
    "    # Apply steering at hidden state\n",
    "    h_steer = h_last + alpha * v.to(h_last.dtype).to(h_last.device)\n",
    "    # Map to logits via the LM head (same as internal final projection)\n",
    "    # HF LLaVA wraps a LlamaForCausalLM under model.language_model\n",
    "    lm_head = model.get_output_embeddings()               # weight tied to vocab\n",
    "    logits = lm_head(h_steer)                             # (1, vocab)\n",
    "    # Compare Yes vs No\n",
    "    yes_logit = logits[0, YES_ID].item()\n",
    "    no_logit  = logits[0, NO_ID].item()\n",
    "    return \"yes\" if yes_logit >= no_logit else \"no\"\n",
    "\n",
    "def infer_yesno_steered(img_path: str, question: str, alpha: float) -> str:\n",
    "    return hf_llava_answer_yesno_steered(processor, model, img_path, question, steer_v, alpha)\n",
    "\n",
    "# Quick smoke on 8 items with steering\n",
    "print(\"Steered smoke (alpha=1.0):\")\n",
    "sm = eval_rows(pope[\"random\"][:8], COCO_IMG_ROOT, lambda p,q: infer_yesno_steered(p,q,alpha=1.0))\n",
    "print(\"SMOKE (steered):\", pct(sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6a9c71c-be9a-45f4-a78d-97083f7bcce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Baseline (no steering) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 88.83, 'Precision': 96.05, 'Recall': 81.0, 'F1': 87.88}\n",
      "- Popular     : {'Accuracy': 86.93, 'Precision': 91.91, 'Recall': 81.0, 'F1': 86.11}\n",
      "- Adversarial : {'Accuracy': 84.03, 'Precision': 86.23, 'Recall': 81.0, 'F1': 83.53}\n",
      "- Overall     : {'Accuracy': 86.6, 'Precision': 91.22, 'Recall': 81.0, 'F1': 85.81}\n",
      "\n",
      "Total time: 2838.0s for 9000 items\n",
      "\n",
      "== Steered (alpha=0.25) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 88.9, 'Precision': 95.84, 'Recall': 81.33, 'F1': 87.99}\n",
      "- Popular     : {'Accuracy': 86.97, 'Precision': 91.66, 'Recall': 81.33, 'F1': 86.19}\n",
      "- Adversarial : {'Accuracy': 84.0, 'Precision': 85.92, 'Recall': 81.33, 'F1': 83.56}\n",
      "- Overall     : {'Accuracy': 86.62, 'Precision': 90.95, 'Recall': 81.33, 'F1': 85.88}\n",
      "\n",
      "Total time: 2279.9s for 9000 items\n",
      "\n",
      "== Steered (alpha=0.5) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 88.9, 'Precision': 95.55, 'Recall': 81.6, 'F1': 88.03}\n",
      "- Popular     : {'Accuracy': 87.0, 'Precision': 91.48, 'Recall': 81.6, 'F1': 86.26}\n",
      "- Adversarial : {'Accuracy': 83.97, 'Precision': 85.65, 'Recall': 81.6, 'F1': 83.58}\n",
      "- Overall     : {'Accuracy': 86.62, 'Precision': 90.71, 'Recall': 81.6, 'F1': 85.91}\n",
      "\n",
      "Total time: 2245.7s for 9000 items\n",
      "\n",
      "== Steered (alpha=1.0) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 89.27, 'Precision': 95.38, 'Recall': 82.53, 'F1': 88.49}\n",
      "- Popular     : {'Accuracy': 86.9, 'Precision': 90.43, 'Recall': 82.53, 'F1': 86.3}\n",
      "- Adversarial : {'Accuracy': 83.7, 'Precision': 84.51, 'Recall': 82.53, 'F1': 83.51}\n",
      "- Overall     : {'Accuracy': 86.62, 'Precision': 89.88, 'Recall': 82.53, 'F1': 86.05}\n",
      "\n",
      "Total time: 2245.0s for 9000 items\n",
      "\n",
      "== Steered (alpha=1.5) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 89.47, 'Precision': 95.05, 'Recall': 83.27, 'F1': 88.77}\n",
      "- Popular     : {'Accuracy': 86.73, 'Precision': 89.47, 'Recall': 83.27, 'F1': 86.26}\n",
      "- Adversarial : {'Accuracy': 83.17, 'Precision': 83.1, 'Recall': 83.27, 'F1': 83.18}\n",
      "- Overall     : {'Accuracy': 86.46, 'Precision': 88.94, 'Recall': 83.27, 'F1': 86.01}\n",
      "\n",
      "Total time: 2327.1s for 9000 items\n",
      "\n",
      "== Summary (Overall) ==\n",
      "baseline: Acc=86.60  F1=85.81\n",
      "alpha=0.25: Acc=86.62  F1=85.88\n",
      "alpha= 0.5: Acc=86.62  F1=85.91\n",
      "alpha= 1.0: Acc=86.62  F1=86.05\n",
      "alpha= 1.5: Acc=86.46  F1=86.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% 13) Full POPE run: baseline vs steered (grid over alpha)\n",
    "import numpy as np, time\n",
    "\n",
    "def run_full_pope_with_alpha(pope, coco_img_root, alpha):\n",
    "    return run_full_pope(\n",
    "        pope, coco_img_root, \n",
    "        infer_fn=lambda p,q: infer_yesno_steered(p,q,alpha)\n",
    "    )\n",
    "\n",
    "# Baseline (already computed earlier as 'results, overall'), but re-run to time-match\n",
    "print(\"\\n== Baseline (no steering) ==\")\n",
    "baseline_res, baseline_overall = run_full_pope(pope, COCO_IMG_ROOT, infer_yesno)\n",
    "\n",
    "# Try a few steering strengths; tune by the overall F1 (or Accuracy).\n",
    "alphas = [0.25, 0.5, 1.0, 1.5]\n",
    "grid = []\n",
    "for a in alphas:\n",
    "    print(f\"\\n== Steered (alpha={a}) ==\")\n",
    "    res_a, overall_a = run_full_pope_with_alpha(pope, COCO_IMG_ROOT, a)\n",
    "    grid.append((a, res_a, overall_a))\n",
    "\n",
    "# Summarize\n",
    "def _brief(o): \n",
    "    x = pct(o); \n",
    "    return f\"Acc={x['Accuracy']:.2f}  F1={x['F1']:.2f}\"\n",
    "\n",
    "print(\"\\n== Summary (Overall) ==\")\n",
    "print(f\"baseline: {_brief(baseline_overall)}\")\n",
    "for a, _, ov in grid:\n",
    "    print(f\"alpha={a:>4}: {_brief(ov)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cb8c724-ca49-4673-8d2c-754a53b5c52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steer_v_lang: (4096,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% A1) Build steering vector: image vs **language-only** (no <image>)\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def _last_hidden_state_langonly(processor, model, question):\n",
    "    prompt = (\n",
    "        \"USER:\\n\"   # <-- no <image> token here\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. \"\n",
    "        \"Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"ASSISTANT:\"\n",
    "    )\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    return out.hidden_states[-1][:, -1, :]  # (1, d)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _last_hidden_state_image(processor, model, image, question):\n",
    "    prompt = (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. \"\n",
    "        \"Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"ASSISTANT:\"\n",
    "    )\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    out = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    return out.hidden_states[-1][:, -1, :]  # (1, d)\n",
    "\n",
    "def build_steer_langonly(pope, coco_img_root, processor, model, split=\"random\", num_calib=400, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    rows = pope[split][:num_calib]\n",
    "    vecs = []\n",
    "    for r in tqdm(rows, desc=\"Calibrating (image v. lang-only)\", leave=False):\n",
    "        p = os.path.join(coco_img_root, r[\"image\"])\n",
    "        if not os.path.exists(p): \n",
    "            continue\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        h_img  = _last_hidden_state_image(processor, model, img, r[\"question\"])\n",
    "        h_lang = _last_hidden_state_langonly(processor, model, r[\"question\"])\n",
    "        vecs.append((h_img - h_lang).cpu())\n",
    "    v = torch.stack(vecs).mean(0).squeeze(0)\n",
    "    v = v / (v.norm(p=2) + 1e-8)\n",
    "    return v\n",
    "\n",
    "steer_v_lang = build_steer_langonly(pope, COCO_IMG_ROOT, processor, model, split=\"random\", num_calib=600)\n",
    "print(\"steer_v_lang:\", tuple(steer_v_lang.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fb3cb14-a11b-4759-bbb5-1dc6199987a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Steered (lang-only, alpha=0.5) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 88.87, 'Precision': 96.27, 'Recall': 80.87, 'F1': 87.9}\n",
      "- Popular     : {'Accuracy': 87.03, 'Precision': 92.24, 'Recall': 80.87, 'F1': 86.18}\n",
      "- Adversarial : {'Accuracy': 84.23, 'Precision': 86.7, 'Recall': 80.87, 'F1': 83.68}\n",
      "- Overall     : {'Accuracy': 86.71, 'Precision': 91.57, 'Recall': 80.87, 'F1': 85.89}\n",
      "\n",
      "Total time: 2241.0s for 9000 items\n",
      "\n",
      "== Steered (lang-only, alpha=1.0) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 88.63, 'Precision': 96.32, 'Recall': 80.33, 'F1': 87.6}\n",
      "- Popular     : {'Accuracy': 86.97, 'Precision': 92.62, 'Recall': 80.33, 'F1': 86.04}\n",
      "- Adversarial : {'Accuracy': 84.17, 'Precision': 87.0, 'Recall': 80.33, 'F1': 83.54}\n",
      "- Overall     : {'Accuracy': 86.59, 'Precision': 91.82, 'Recall': 80.33, 'F1': 85.69}\n",
      "\n",
      "Total time: 2263.1s for 9000 items\n",
      "\n",
      "== Steered (lang-only, alpha=1.5) ==\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) results:\n",
      "- Random      : {'Accuracy': 88.4, 'Precision': 96.45, 'Recall': 79.73, 'F1': 87.3}\n",
      "- Popular     : {'Accuracy': 86.73, 'Precision': 92.71, 'Recall': 79.73, 'F1': 85.73}\n",
      "- Adversarial : {'Accuracy': 84.03, 'Precision': 87.24, 'Recall': 79.73, 'F1': 83.32}\n",
      "- Overall     : {'Accuracy': 86.39, 'Precision': 91.98, 'Recall': 79.73, 'F1': 85.42}\n",
      "\n",
      "Total time: 2287.7s for 9000 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% A2) Re-run with the new vector (try a small alpha grid)\n",
    "def infer_yesno_steered_lang(img_path, q, alpha=1.0):\n",
    "    return hf_llava_answer_yesno_steered(processor, model, img_path, q, steer_v_lang, alpha)\n",
    "\n",
    "for a in [0.5, 1.0, 1.5]:\n",
    "    print(f\"\\n== Steered (lang-only, alpha={a}) ==\")\n",
    "    _ = run_full_pope(pope, COCO_IMG_ROOT, lambda p,q: infer_yesno_steered_lang(p,q,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31de722b-00aa-46a1-854d-7898a1bbaed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 decoder layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid-layer v: (4096,) layer idx: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% B1) Capture a layer's output and build v there (robust to HF variants)\n",
    "import contextlib, torch\n",
    "from PIL import Image\n",
    "\n",
    "def _get_llama_layers_from_llava(model):\n",
    "    \"\"\"\n",
    "    Return the list of decoder layers (LlamaDecoderLayer) robustly across HF LLaVA variants.\n",
    "    \"\"\"\n",
    "    # Common: LlavaForConditionalGeneration has .language_model as LlamaForCausalLM\n",
    "    if hasattr(model, \"language_model\"):\n",
    "        lm = model.language_model\n",
    "        # Case A: language_model is LlamaForCausalLM with .model (LlamaModel)\n",
    "        if hasattr(lm, \"model\") and hasattr(lm.model, \"layers\"):\n",
    "            return lm.model.layers\n",
    "        # Case B: language_model is already LlamaModel\n",
    "        if hasattr(lm, \"layers\"):\n",
    "            return lm.layers\n",
    "    # Fallbacks (rare)\n",
    "    if hasattr(model, \"model\") and hasattr(model.model, \"layers\"):\n",
    "        return model.model.layers\n",
    "    raise AttributeError(\"Could not locate LLaMA decoder layers in this HF LLaVA build.\")\n",
    "\n",
    "LAYERS = _get_llama_layers_from_llava(model)\n",
    "NUM_LAYERS = len(LAYERS)\n",
    "print(f\"Found {NUM_LAYERS} decoder layers.\")\n",
    "STEER_LAYER_IDX = min(28, NUM_LAYERS - 1)  # pick a high-ish layer safely\n",
    "\n",
    "def _forward_with_layer_capture(inputs, layer_idx):\n",
    "    \"\"\"\n",
    "    Run a forward pass and capture the output of the chosen decoder layer.\n",
    "    Returns tensor of shape (1, hidden_size) for the **last token**.\n",
    "    \"\"\"\n",
    "    captured = {}\n",
    "    target_layer = LAYERS[layer_idx]\n",
    "\n",
    "    def hook_fn(module, inp, out):\n",
    "        # out: (batch, seq, hidden) -- we take the last token\n",
    "        captured[\"h\"] = out[:, -1, :].detach()\n",
    "        return out\n",
    "\n",
    "    with torch.no_grad(), contextlib.ExitStack() as stack:\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        stack.enter_context(handle)\n",
    "        _ = model(**inputs, output_hidden_states=False, use_cache=False)\n",
    "    return captured.get(\"h\")  # (1, hidden)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _layer_hidden_image(question, image, layer_idx):\n",
    "    prompt = (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    return _forward_with_layer_capture(inputs, layer_idx)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _layer_hidden_langonly(question, layer_idx):\n",
    "    # no <image> → pure language prior\n",
    "    prompt = (\n",
    "        \"USER:\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    return _forward_with_layer_capture(inputs, layer_idx)\n",
    "\n",
    "def build_midlayer_vector(pope, coco_img_root, layer_idx=STEER_LAYER_IDX, num_calib=600):\n",
    "    vecs = []\n",
    "    rows = pope[\"random\"][:num_calib]\n",
    "    for r in tqdm(rows, desc=f\"Build v @ layer {layer_idx}\", leave=False):\n",
    "        p = os.path.join(coco_img_root, r[\"image\"])\n",
    "        if not os.path.exists(p): \n",
    "            continue\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        h_img  = _layer_hidden_image(r[\"question\"], img, layer_idx)\n",
    "        h_lang = _layer_hidden_langonly(r[\"question\"], layer_idx)\n",
    "        if h_img is None or h_lang is None:\n",
    "            continue\n",
    "        vecs.append((h_img - h_lang).cpu())\n",
    "    if not vecs:\n",
    "        raise RuntimeError(\"No calibration vectors collected — check image paths/split names.\")\n",
    "    v = torch.stack(vecs).mean(0).squeeze(0)\n",
    "    v = v / (v.norm(p=2) + 1e-8)\n",
    "    return v\n",
    "\n",
    "steer_v_mid = build_midlayer_vector(pope, COCO_IMG_ROOT, layer_idx=STEER_LAYER_IDX, num_calib=600)\n",
    "print(\"mid-layer v:\", tuple(steer_v_mid.shape), \"layer idx:\", STEER_LAYER_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73b0c691-420c-416e-af62-7c2226778d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid-layer steered smoke:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 100.0, 'Precision': 100.0, 'Recall': 100.0, 'F1': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% B2) Inference with mid-layer steering (hook adds α·v to chosen layer's output)\n",
    "import contextlib\n",
    "\n",
    "@torch.no_grad()\n",
    "def hf_llava_yesno_midlayer_steer(img_path, question, layer_idx, v, alpha=1.0):\n",
    "    prompt = (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    target_layer = LAYERS[layer_idx]\n",
    "\n",
    "    def hook_fn(module, inp, out):\n",
    "        # out: (batch, seq, hidden) → add α·v to all time-steps (cheap), or only last:\n",
    "        add = alpha * v.to(out.dtype).to(out.device)\n",
    "        return out + add  # broadcast over (batch, seq, hidden)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=8,\n",
    "        )\n",
    "        handle.remove()\n",
    "\n",
    "    out_txt = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    return normalize_yesno(out_txt)\n",
    "\n",
    "def infer_yesno_mid(img_path, q, alpha=1.0):\n",
    "    return hf_llava_yesno_midlayer_steer(img_path, q, STEER_LAYER_IDX, steer_v_mid, alpha)\n",
    "\n",
    "# Quick sanity on a few samples\n",
    "print(\"Mid-layer steered smoke:\")\n",
    "print(pct(eval_rows(pope[\"random\"][:16], COCO_IMG_ROOT, lambda p,q: infer_yesno_mid(p,q,alpha=1.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69720322-35ac-4b69-8eda-cff671ec5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Full-cycle runner for mid-layer steering\n",
    "import time, csv\n",
    "\n",
    "def run_full_pope_with_mid_steer(pope, coco_img_root, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Uses infer_yesno_mid(img_path, q, alpha) which you validated in B2.\n",
    "    Prints per-split + overall metrics and returns (results_dict, overall_dict).\n",
    "    \"\"\"\n",
    "    def infer_fn(p, q): \n",
    "        return infer_yesno_mid(p, q, alpha=alpha)\n",
    "\n",
    "    t0 = time.time()\n",
    "    results = {}\n",
    "    all_rows = []\n",
    "    for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"\\nRunning split (alpha={alpha}): {split}\")\n",
    "        s = eval_rows(pope[split], coco_img_root, infer_fn)\n",
    "        results[split] = s\n",
    "        all_rows.extend(pope[split])\n",
    "    overall = eval_rows(all_rows, coco_img_root, infer_fn)\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(\"\\nPOPE (COCO) mid-layer steered results:\")\n",
    "    for k in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"- {k.title():12}:\", pct(results[k]))\n",
    "    print(f\"- Overall     :\", pct(overall))\n",
    "    print(f\"\\nTotal time: {t1 - t0:.1f}s for {sum(len(pope[s]) for s in ('random','popular','adversarial'))} items\")\n",
    "    return results, overall\n",
    "\n",
    "def save_predictions_mid(pope, coco_img_root, out_csv, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Saves per-question predictions for the steered run.\n",
    "    \"\"\"\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"split\",\"image\",\"question\",\"gt_answer\",\"pred_answer\",\"alpha\",\"layer_idx\"])\n",
    "        for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "            for r in tqdm(pope[split], leave=False, desc=f\"Saving {split} (alpha={alpha})\"):\n",
    "                img_file = r[\"image\"]\n",
    "                img_path = img_file if os.path.isabs(img_file) else os.path.join(coco_img_root, img_file)\n",
    "                if not os.path.exists(img_path):\n",
    "                    continue\n",
    "                pred = infer_yesno_mid(img_path, r[\"question\"], alpha=alpha)\n",
    "                w.writerow([split, img_file, r[\"question\"], r[\"answer\"], pred, alpha, STEER_LAYER_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ade7a3ba-6a91-433e-a56d-65e0b7a61fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split (alpha=1.0): random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split (alpha=1.0): popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split (alpha=1.0): adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) mid-layer steered results:\n",
      "- Random      : {'Accuracy': 88.9, 'Precision': 96.27, 'Recall': 80.93, 'F1': 87.94}\n",
      "- Popular     : {'Accuracy': 87.07, 'Precision': 92.25, 'Recall': 80.93, 'F1': 86.22}\n",
      "- Adversarial : {'Accuracy': 84.27, 'Precision': 86.71, 'Recall': 80.93, 'F1': 83.72}\n",
      "- Overall     : {'Accuracy': 86.74, 'Precision': 91.58, 'Recall': 80.93, 'F1': 85.93}\n",
      "\n",
      "Total time: 2735.2s for 9000 items\n",
      "\n",
      "Overall (mid-layer, alpha = 1.0 ): {'Accuracy': 86.74, 'Precision': 91.58, 'Recall': 80.93, 'F1': 85.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %% Run one full steered evaluation (set your alpha here)\n",
    "ALPHA = 1.0  # try 0.5, 1.0, 1.5, 2.0\n",
    "results_mid, overall_mid = run_full_pope_with_mid_steer(pope, COCO_IMG_ROOT, alpha=ALPHA)\n",
    "print(\"\\nOverall (mid-layer, alpha =\", ALPHA, \"):\", pct(overall_mid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1733b48-d1c0-4d32-9695-52b5f7880ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_yesno_compat(s: str) -> str:\n",
    "    \"\"\"Return exactly 'yes' or 'no' to match eval_rows expectations.\"\"\"\n",
    "    t = (s or \"\").strip().lower()\n",
    "    # common variants\n",
    "    if t.startswith(\"y\"): return \"yes\"\n",
    "    if t.startswith(\"n\"): return \"no\"\n",
    "    # fallback: search tokens\n",
    "    if \"yes\" in t: return \"yes\"\n",
    "    if \"no\"  in t: return \"no\"\n",
    "    # absolute fallback: default to 'no'\n",
    "    return \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90f7f436-a033-449a-8e7c-e0c32031c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, os\n",
    "from PIL import Image\n",
    "\n",
    "# Map output text to strict Yes/No\n",
    "def strict_yesno(text: str) -> str:\n",
    "    t = text.strip().lower()\n",
    "    return \"Yes\" if t.startswith(\"y\") else \"No\"\n",
    "\n",
    "# Build a Yes/No-only prompt (same style as yours)\n",
    "def yn_prompt(question: str) -> str:\n",
    "    return (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "\n",
    "# Token IDs for \"yes\"/\"no\" (robust to BPE variants)\n",
    "def get_yes_no_ids(tokenizer):\n",
    "    yes_ids = [tokenizer.convert_tokens_to_ids(\"Yes\"),\n",
    "               tokenizer.convert_tokens_to_ids(\"yes\")]\n",
    "    no_ids  = [tokenizer.convert_tokens_to_ids(\"No\"),\n",
    "               tokenizer.convert_tokens_to_ids(\"no\")]\n",
    "    yes_ids  = [i for i in yes_ids if i is not None and i >= 0]\n",
    "    no_ids   = [i for i in no_ids  if i is not None and i >= 0]\n",
    "    # Fallback: first-subtoken of the string\n",
    "    if not yes_ids:\n",
    "        yes_ids = [tokenizer(\"Yes\", add_special_tokens=False).input_ids[0]]\n",
    "    if not no_ids:\n",
    "        no_ids = [tokenizer(\"No\", add_special_tokens=False).input_ids[0]]\n",
    "    return list(dict.fromkeys(yes_ids)), list(dict.fromkeys(no_ids))\n",
    "\n",
    "YES_IDS, NO_IDS = get_yes_no_ids(processor.tokenizer)\n",
    "\n",
    "# Get logits for the last generated position without sampling\n",
    "@torch.no_grad()\n",
    "def get_last_logits(model, inputs):\n",
    "    out = model(**inputs, use_cache=False, return_dict=True, output_hidden_states=False)\n",
    "    # last position in the prompt; we want NEXT-token distribution, so take last logits row\n",
    "    return out.logits[:, -1, :]  # (B,V)\n",
    "\n",
    "# Convert yes/no logits to probability\n",
    "def yes_prob_from_logits(logits, yes_ids=YES_IDS, no_ids=NO_IDS):\n",
    "    # pool over possible tokenizations\n",
    "    yes_logit = logits[..., yes_ids].logsumexp(dim=-1)\n",
    "    no_logit  = logits[...,  no_ids].logsumexp(dim=-1)\n",
    "    # binary softmax\n",
    "    m = torch.stack([yes_logit, no_logit], dim=-1)\n",
    "    p = torch.softmax(m, dim=-1)[..., 0]\n",
    "    return p  # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b79da28c-3571-4013-acad-3ce1aef971fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1 Safer mid-layer hook (last-token only + recentre)\n",
    "class MidLayerSteerer:\n",
    "    def __init__(self, v, alpha_base=0.8, last_token_only=True, recenter=True):\n",
    "        v = v.float()\n",
    "        self.v = v / (v.norm(p=2) + 1e-8)            # unit direction\n",
    "        self.alpha_base = float(alpha_base)\n",
    "        self.last_token_only = last_token_only\n",
    "        self.recenter = recenter\n",
    "        self.handle = None\n",
    "        self.gates = None  # set per batch\n",
    "\n",
    "    def _hook(self, module, inp, out):\n",
    "        # out: (B, T, D)\n",
    "        B, T, D = out.shape\n",
    "        v = self.v.to(out.device, out.dtype).view(1,1,D)\n",
    "        g = self.gates\n",
    "        if g is None:\n",
    "            g = torch.full((B,1,1), 0.25, dtype=out.dtype, device=out.device)\n",
    "        else:\n",
    "            g = g.view(B,1,1).to(out.dtype).to(out.device)\n",
    "        add = self.alpha_base * g * v\n",
    "        if self.last_token_only:\n",
    "            mask = torch.zeros((B,T,1), dtype=out.dtype, device=out.device)\n",
    "            mask[:, -1, :] = 1.0\n",
    "            out = out + add * mask\n",
    "        else:\n",
    "            out = out + add\n",
    "        if self.recenter:\n",
    "            out = out - out.mean(dim=-1, keepdim=True)\n",
    "        return out\n",
    "\n",
    "    def attach(self, transformer_block):\n",
    "        self.handle = transformer_block.register_forward_hook(self._hook)\n",
    "\n",
    "    def detach(self):\n",
    "        if self.handle is not None:\n",
    "            self.handle.remove()\n",
    "            self.handle = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91b404a0-9d33-4769-8272-1f7cb7b95b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_yesno_gated_mid(img_path, question, layer_idx=None, v=None, alpha_base=0.8):\n",
    "    assert v is not None, \"Provide the steering vector v (e.g., steer_v_mid).\"\n",
    "    LIDX = STEER_LAYER_IDX if layer_idx is None else layer_idx\n",
    "    block = LAYERS[LIDX]\n",
    "\n",
    "    from PIL import Image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question)\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    gate = compute_gate_for_item(model, processor, img, question)\n",
    "\n",
    "    steerer = MidLayerSteerer(v, alpha_base=alpha_base, last_token_only=True, recenter=True)\n",
    "    steerer.gates = torch.tensor([gate], device=DEVICE)\n",
    "\n",
    "    handle = None\n",
    "    try:\n",
    "        handle = block.register_forward_hook(steerer._hook)\n",
    "        out_ids = model.generate(**inputs, do_sample=False, max_new_tokens=8)\n",
    "    finally:\n",
    "        if handle is not None:\n",
    "            handle.remove()\n",
    "\n",
    "    txt = processor.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "    return normalize_yesno_compat(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c2dd6d8-541e-44aa-80cb-eeea0798c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gated-mid smoke acc: {'Accuracy': 50.0, 'Precision': 50.0, 'Recall': 100.0, 'F1': 66.67}\n",
      "Pred distribution: Counter({'yes': 16})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from collections import Counter\n",
    "smoke_preds = [infer_yesno_gated_mid(os.path.join(COCO_IMG_ROOT, r[\"image\"]), r[\"question\"], v=steer_v_mid, alpha_base=0.8)\n",
    "               for r in pope[\"random\"][:16]]\n",
    "print(\"Gated-mid smoke acc:\", pct(eval_rows(pope[\"random\"][:16], COCO_IMG_ROOT,\n",
    "      lambda p,q: infer_yesno_gated_mid(p,q, v=steer_v_mid, alpha_base=0.8))))\n",
    "print(\"Pred distribution:\", Counter(smoke_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31eeeb45-1727-4e33-b629-8a42c91f0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit-bias smoke acc: {'Accuracy': 50.0, 'Precision': 50.0, 'Recall': 100.0, 'F1': 66.67}\n",
      "Pred distribution: Counter({'yes': 16})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# B1. Logit hook + inference\n",
    "class YesNoLogitBias:\n",
    "    def __init__(self, tokenizer, bias_yes=-0.25, bias_no=+0.05):\n",
    "        self.bias_yes = float(bias_yes)\n",
    "        self.bias_no  = float(bias_no)\n",
    "        self.yes_ids, self.no_ids = get_yes_no_ids(tokenizer)\n",
    "        self.handle = None\n",
    "\n",
    "    def _hook(self, module, inp, out):\n",
    "        # out: (B,T,V)\n",
    "        if self.bias_yes != 0:\n",
    "            out[..., self.yes_ids] = out[..., self.yes_ids] + self.bias_yes\n",
    "        if self.bias_no != 0:\n",
    "            out[..., self.no_ids]  = out[..., self.no_ids]  + self.bias_no\n",
    "        return out\n",
    "\n",
    "    def attach(self, lm_head):\n",
    "        self.handle = lm_head.register_forward_hook(self._hook)\n",
    "\n",
    "    def detach(self):\n",
    "        if self.handle: \n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_yesno_logit_bias(img_path, question, bias_yes=-0.25, bias_no=+0.05):\n",
    "    prompt = yn_prompt(question)\n",
    "    from PIL import Image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    def _hook(module, inp, out):\n",
    "        # out: (B,T,V)\n",
    "        yes_ids, no_ids = YES_IDS, NO_IDS\n",
    "        if bias_yes != 0:\n",
    "            out[..., yes_ids] = out[..., yes_ids] + bias_yes\n",
    "        if bias_no != 0:\n",
    "            out[..., no_ids]  = out[..., no_ids]  + bias_no\n",
    "        return out\n",
    "\n",
    "    handle = None\n",
    "    try:\n",
    "        handle = model.lm_head.register_forward_hook(_hook)\n",
    "        out_ids = model.generate(**inputs, do_sample=False, max_new_tokens=8)\n",
    "    finally:\n",
    "        if handle: handle.remove()\n",
    "\n",
    "    txt = processor.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "    return normalize_yesno_compat(txt)\n",
    "\n",
    "# Smoke\n",
    "from collections import Counter\n",
    "smoke_preds = [infer_yesno_logit_bias(os.path.join(COCO_IMG_ROOT, r[\"image\"]), r[\"question\"])\n",
    "               for r in pope[\"random\"][:16]]\n",
    "print(\"Logit-bias smoke acc:\", pct(eval_rows(pope[\"random\"][:16], COCO_IMG_ROOT,\n",
    "      lambda p,q: infer_yesno_logit_bias(p,q))))\n",
    "print(\"Pred distribution:\", Counter(smoke_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6db18681-3c94-43cf-a4cf-9d0f7b9892b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1. Collect p(Yes) on a dev slice and fit τ\n",
    "@torch.no_grad()\n",
    "def yes_prob_unsteered(img_path, question):\n",
    "    prompt = yn_prompt(question)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits = get_last_logits(model, inputs)  # (1,V)\n",
    "    p = yes_prob_from_logits(logits)         # (1,)\n",
    "    return float(p.item())\n",
    "\n",
    "def fit_tau(dev_rows, coco_img_root, taus=None, metric=\"accuracy\"):\n",
    "    if taus is None:\n",
    "        taus = np.linspace(0.30, 0.70, 81)  # 0.30..0.70 step 0.005\n",
    "    p_yes, y = [], []\n",
    "    for r in dev_rows:\n",
    "        img_file = r[\"image\"]\n",
    "        img_path = img_file if os.path.isabs(img_file) else os.path.join(coco_img_root, img_file)\n",
    "        if not os.path.exists(img_path): \n",
    "            continue\n",
    "        p = yes_prob_unsteered(img_path, r[\"question\"])\n",
    "        p_yes.append(p)\n",
    "        y.append(1 if r[\"answer\"].lower() == \"yes\" else 0)\n",
    "    p_yes = np.array(p_yes); y = np.array(y)\n",
    "\n",
    "    best_tau, best_val = 0.5, -1.0\n",
    "    for t in taus:\n",
    "        yhat = (p_yes > t).astype(int)\n",
    "        if metric == \"f1\":\n",
    "            tp = ((yhat==1)&(y==1)).sum(); fp = ((yhat==1)&(y==0)).sum(); fn = ((yhat==0)&(y==1)).sum()\n",
    "            prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9)\n",
    "            val = 2*prec*rec/(prec+rec+1e-9)\n",
    "        else:\n",
    "            val = (yhat == y).mean()\n",
    "        if val > best_val:\n",
    "            best_val, best_tau = val, t\n",
    "    return float(best_tau), float(best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6991c53b-9fd0-42ad-9efd-a05be845bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted τ ≈ 0.300 (dev acc 0.886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold smoke acc: {'Accuracy': 100.0, 'Precision': 100.0, 'Recall': 100.0, 'F1': 100.0}\n",
      "Pred distribution: Counter({'yes': 8, 'no': 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def infer_yesno_thresholded(img_path, question, tau):\n",
    "    p = yes_prob_unsteered(img_path, question)\n",
    "    return \"yes\" if p > tau else \"no\"\n",
    "\n",
    "# Smoke\n",
    "from collections import Counter\n",
    "# Fit τ on a small mixed slice (adjust N if needed)\n",
    "dev_mix = pope[\"random\"][:120] + pope[\"popular\"][:120] + pope[\"adversarial\"][:120]\n",
    "tau_star, val = fit_tau(dev_mix, COCO_IMG_ROOT, metric=\"accuracy\")\n",
    "print(f\"Fitted τ ≈ {tau_star:.3f} (dev acc {val:.3f})\")\n",
    "smoke_preds = [infer_yesno_thresholded(os.path.join(COCO_IMG_ROOT, r[\"image\"]), r[\"question\"], tau_star)\n",
    "               for r in pope[\"random\"][:16]]\n",
    "print(\"Threshold smoke acc:\", pct(eval_rows(pope[\"random\"][:16], COCO_IMG_ROOT,\n",
    "      lambda p,q: infer_yesno_thresholded(p,q,tau_star))))\n",
    "print(\"Pred distribution:\", Counter(smoke_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f432eff6-5dab-463e-821e-c07512ab533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, numpy as np\n",
    "from PIL import Image\n",
    "import torch, csv\n",
    "\n",
    "def yn_prompt(question: str) -> str:\n",
    "    return (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_last_logits(model, inputs):\n",
    "    out = model(**inputs, use_cache=False, return_dict=True, output_hidden_states=False)\n",
    "    return out.logits[:, -1, :]  # (B,V)\n",
    "\n",
    "# Robust “yes/no” token pooling for probability\n",
    "def _get_yes_no_ids(tokenizer):\n",
    "    yes_ids = [tokenizer.convert_tokens_to_ids(\"Yes\"),\n",
    "               tokenizer.convert_tokens_to_ids(\"yes\")]\n",
    "    no_ids  = [tokenizer.convert_tokens_to_ids(\"No\"),\n",
    "               tokenizer.convert_tokens_to_ids(\"no\")]\n",
    "    yes_ids = [i for i in yes_ids if i is not None and i >= 0]\n",
    "    no_ids  = [i for i in no_ids  if i is not None and i >= 0]\n",
    "    if not yes_ids:\n",
    "        yes_ids = [tokenizer(\"Yes\", add_special_tokens=False).input_ids[0]]\n",
    "    if not no_ids:\n",
    "        no_ids  = [tokenizer(\"No\",  add_special_tokens=False).input_ids[0]]\n",
    "    # dedupe\n",
    "    yes_ids = list(dict.fromkeys(yes_ids)); no_ids = list(dict.fromkeys(no_ids))\n",
    "    return yes_ids, no_ids\n",
    "\n",
    "YES_IDS, NO_IDS = _get_yes_no_ids(processor.tokenizer)\n",
    "\n",
    "def yes_prob_from_logits(logits, yes_ids=YES_IDS, no_ids=NO_IDS):\n",
    "    yes_logit = logits[..., yes_ids].logsumexp(dim=-1)\n",
    "    no_logit  = logits[...,  no_ids].logsumexp(dim=-1)\n",
    "    m = torch.stack([yes_logit, no_logit], dim=-1)\n",
    "    return torch.softmax(m, dim=-1)[..., 0]  # p(yes)\n",
    "\n",
    "@torch.no_grad()\n",
    "def yes_prob_unsteered(img_path, question):\n",
    "    prompt = yn_prompt(question)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits = get_last_logits(model, inputs)\n",
    "    p = yes_prob_from_logits(logits)\n",
    "    return float(p.item())\n",
    "\n",
    "def fit_tau(dev_rows, coco_img_root, taus=None, metric=\"accuracy\"):\n",
    "    if taus is None:\n",
    "        taus = np.linspace(0.30, 0.70, 81)  # 0.30..0.70 step 0.005\n",
    "    p_yes, y = [], []\n",
    "    for r in dev_rows:\n",
    "        img_file = r[\"image\"]\n",
    "        img_path = img_file if os.path.isabs(img_file) else os.path.join(coco_img_root, img_file)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        p = yes_prob_unsteered(img_path, r[\"question\"])\n",
    "        p_yes.append(p)\n",
    "        y.append(1 if r[\"answer\"].lower().startswith(\"y\") else 0)\n",
    "    p_yes = np.array(p_yes); y = np.array(y)\n",
    "    best_tau, best_val = 0.5, -1.0\n",
    "    for t in taus:\n",
    "        yhat = (p_yes > t).astype(int)\n",
    "        if metric == \"f1\":\n",
    "            tp = ((yhat==1)&(y==1)).sum(); fp = ((yhat==1)&(y==0)).sum(); fn = ((yhat==0)&(y==1)).sum()\n",
    "            prec = tp/(tp+fp+1e-9); rec = tp/(tp+fn+1e-9)\n",
    "            val = 2*prec*rec/(prec+rec+1e-9)\n",
    "        else:\n",
    "            val = (yhat == y).mean()\n",
    "        if val > best_val:\n",
    "            best_val, best_tau = val, t\n",
    "    return float(best_tau), float(best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a73356ac-917b-436d-8a97-950c2c2fddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_yesno_thresholded(img_path, question, tau):\n",
    "    p = yes_prob_unsteered(img_path, question)\n",
    "    return \"yes\" if p > tau else \"no\"\n",
    "\n",
    "def run_pope_thresholded(pope, coco_img_root, tau):\n",
    "    def infer_fn(p,q): return infer_yesno_thresholded(p,q,tau)\n",
    "    t0 = time.time()\n",
    "    res = {}\n",
    "    all_rows = []\n",
    "    for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"\\nRunning split [Threshold τ={tau:.3f}]: {split}\")\n",
    "        s = eval_rows(pope[split], coco_img_root, infer_fn)\n",
    "        res[split] = s\n",
    "        all_rows.extend(pope[split])\n",
    "    overall = eval_rows(all_rows, coco_img_root, infer_fn)\n",
    "    dt = time.time()-t0\n",
    "    print(\"\\nPOPE (COCO) Thresholded results:\")\n",
    "    for k in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"- {k.title():12}:\", pct(res[k]))\n",
    "    print(f\"- Overall     :\", pct(overall))\n",
    "    print(f\"\\nTotal time: {dt:.1f}s for {sum(len(pope[s]) for s in ('random','popular','adversarial'))} items\")\n",
    "    return res, overall\n",
    "\n",
    "def save_predictions_thresholded(pope, coco_img_root, out_csv, tau):\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"split\",\"image\",\"question\",\"gt_answer\",\"pred_answer\",\"tau\"])\n",
    "        for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "            for r in pope[split]:\n",
    "                img_file = r[\"image\"]\n",
    "                img_path = img_file if os.path.isabs(img_file) else os.path.join(coco_img_root, img_file)\n",
    "                if not os.path.exists(img_path):\n",
    "                    continue\n",
    "                pred = infer_yesno_thresholded(img_path, r[\"question\"], tau)\n",
    "                w.writerow([split, img_file, r[\"question\"], r[\"answer\"], pred, tau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "218fb60d-1374-4a4c-8f94-6d716ed07e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted τ ≈ 0.300 on dev (acc 0.872)\n",
      "\n",
      "Running split [Threshold τ=0.300]: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split [Threshold τ=0.300]: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running split [Threshold τ=0.300]: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POPE (COCO) Thresholded results:\n",
      "- Random      : {'Accuracy': 89.53, 'Precision': 95.27, 'Recall': 83.2, 'F1': 88.83}\n",
      "- Popular     : {'Accuracy': 87.0, 'Precision': 90.04, 'Recall': 83.2, 'F1': 86.49}\n",
      "- Adversarial : {'Accuracy': 83.4, 'Precision': 83.53, 'Recall': 83.2, 'F1': 83.37}\n",
      "- Overall     : {'Accuracy': 86.64, 'Precision': 89.36, 'Recall': 83.2, 'F1': 86.17}\n",
      "\n",
      "Total time: 2210.1s for 9000 items\n",
      "\n",
      "Overall (thresholded, τ = 0.300 ): {'Accuracy': 86.64, 'Precision': 89.36, 'Recall': 83.2, 'F1': 86.17}\n",
      "Saved: pope_predictions_thresholded_tau0.300.csv\n"
     ]
    }
   ],
   "source": [
    "# Pick a mixed calibration slice (tweak counts to your liking)\n",
    "N_DEV_PER_SPLIT = 300  # try 200–400; larger is stabler but slower\n",
    "dev_mix = pope[\"random\"][:N_DEV_PER_SPLIT] + pope[\"popular\"][:N_DEV_PER_SPLIT] + pope[\"adversarial\"][:N_DEV_PER_SPLIT]\n",
    "\n",
    "tau_star, dev_score = fit_tau(dev_mix, COCO_IMG_ROOT, metric=\"accuracy\")\n",
    "print(f\"Fitted τ ≈ {tau_star:.3f} on dev (acc {dev_score:.3f})\")\n",
    "\n",
    "# Full evaluation\n",
    "results_thr, overall_thr = run_pope_thresholded(pope, COCO_IMG_ROOT, tau=tau_star)\n",
    "print(\"\\nOverall (thresholded, τ =\", f\"{tau_star:.3f}\", \"):\", pct(overall_thr))\n",
    "\n",
    "# Optional: save per-question predictions\n",
    "OUT_CSV = f\"pope_predictions_thresholded_tau{tau_star:.3f}.csv\"\n",
    "save_predictions_thresholded(pope, COCO_IMG_ROOT, OUT_CSV, tau_star)\n",
    "print(\"Saved:\", OUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb598ed-6c0e-4e92-a906-6d7403b491d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llava)",
   "language": "python",
   "name": "llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
