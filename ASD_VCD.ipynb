{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aec54eb-a0c9-451d-8448-e953a9bb551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahjc\\anaconda3\\envs\\llava\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ahjc\\anaconda3\\envs\\llava\\lib\\site-packages\\torch\\backends\\__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x167b8f25870>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports / globals\n",
    "import os, math, random, time, contextlib, itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Throughput: enable TF32 on RTX 30/40 or A100 class GPUs\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "random.seed(0); torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c432381e-3446-45e1-a767-b17337886dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 16:41:31) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch: 2.9.0+cu126\n",
      "Transformers: 4.57.1\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "try:\n",
    "    import torch, transformers\n",
    "    from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"Transformers:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    print(\"Import check failed:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd8f663-aec6-43d4-9a00-db0e995da0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "MODEL: llava-hf/llava-1.5-7b-hf\n",
      "POPE_ROOT: POPE/output/coco\n",
      "COCO_IMG_ROOT: val2014\n"
     ]
    }
   ],
   "source": [
    "# Config \n",
    "POPE_ROOT      = \"POPE/output/coco\"\n",
    "COCO_IMG_ROOT  = \"val2014\"\n",
    "HF_MODEL_ID    = \"llava-hf/llava-1.5-7b-hf\"\n",
    "DEVICE         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE          = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "MAX_NEW_TOKENS = 8\n",
    "TEMPERATURE    = 0.0\n",
    "CSV_OUT        = \"pope_coco_predictions.csv\"\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"MODEL:\", HF_MODEL_ID)\n",
    "print(\"POPE_ROOT:\", POPE_ROOT)\n",
    "print(\"COCO_IMG_ROOT:\", COCO_IMG_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec006b73-1f81-43d6-afe2-73af08e014c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, os\n",
    "\n",
    "YES_TOKENS = {\"1\",\"true\",\"yes\",\"y\",\"present\",\"a\"}   # include 'a' as some datasets use A/B\n",
    "NO_TOKENS  = {\"0\",\"false\",\"no\",\"n\",\"absent\",\"b\"}\n",
    "\n",
    "def _read_any(path: pathlib.Path):\n",
    "    txt = path.read_text(encoding=\"utf-8\").strip()\n",
    "    try:\n",
    "        obj = json.loads(txt)\n",
    "        if isinstance(obj, dict) and \"data\" in obj:\n",
    "            return obj[\"data\"]\n",
    "        if isinstance(obj, list):\n",
    "            return obj\n",
    "        if isinstance(obj, dict):\n",
    "            return [obj]\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # JSONL fallback\n",
    "    return [json.loads(ln) for ln in txt.splitlines() if ln.strip()]\n",
    "\n",
    "def _canon_answer(row):\n",
    "    raw = None\n",
    "    for k in (\"answer\",\"label\",\"gt\",\"target\",\"ans\",\"response\",\"y\"):\n",
    "        if k in row:\n",
    "            raw = row[k]\n",
    "            break\n",
    "    if raw is None:\n",
    "        return None\n",
    "    # normalize types\n",
    "    if isinstance(raw, bool):\n",
    "        return \"yes\" if raw else \"no\"\n",
    "    if isinstance(raw, (int, float)):\n",
    "        return \"yes\" if int(raw) == 1 else \"no\"\n",
    "    s = str(raw).strip().lower()\n",
    "    if s in YES_TOKENS: return \"yes\"\n",
    "    if s in NO_TOKENS:  return \"no\"\n",
    "    if s in {\"a\",\"b\"}:\n",
    "        # heuristic: map A->yes, B->no unless 'options' says otherwise\n",
    "        opts = row.get(\"options\") or row.get(\"choices\")\n",
    "        if isinstance(opts, dict):\n",
    "            inv = {v.strip().lower(): k.lower() for k,v in opts.items()}\n",
    "            # if options say 'yes' maps to 'a' explicitly, honor that\n",
    "            if \"yes\" in inv and \"no\" in inv:\n",
    "                return \"yes\" if s == inv[\"yes\"] else \"no\"\n",
    "        return \"yes\" if s == \"a\" else \"no\"\n",
    "    return None\n",
    "\n",
    "def _to_val2014_filename(x):\n",
    "    if isinstance(x, int):\n",
    "        return f\"COCO_val2014_{x:012d}.jpg\"\n",
    "    if isinstance(x, str):\n",
    "        return os.path.basename(x)\n",
    "    return None\n",
    "\n",
    "def _canon_image(row):\n",
    "    for k in (\"image\",\"image_id\",\"img\",\"img_id\",\"img_path\",\"file_name\",\"filename\",\"path\"):\n",
    "        if k in row:\n",
    "            v = row[k]\n",
    "            fn = _to_val2014_filename(v)\n",
    "            if fn:\n",
    "                return fn\n",
    "    return None\n",
    "\n",
    "def _canon_question(row):\n",
    "    for k in (\"question\",\"q\",\"text\",\"prompt\",\"instruction\"):\n",
    "        if k in row:\n",
    "            v = row[k]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip()\n",
    "    return None\n",
    "\n",
    "def _pick(pope_root: pathlib.Path, key_substr: str):\n",
    "    cands = [p for p in pope_root.rglob(\"*.json\")] + [p for p in pope_root.rglob(\"*.jsonl\")]\n",
    "    cands = [p for p in cands if key_substr.lower() in p.name.lower() and \"checkpoint\" not in p.name.lower()]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No file containing '{key_substr}' under {pope_root}\")\n",
    "    cands.sort(key=lambda p: p.stat().st_size, reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def load_pope_split_robust(pope_root: str) -> dict:\n",
    "    root = pathlib.Path(pope_root)\n",
    "    assert root.exists(), f\"POPE_ROOT not found: {root}\"\n",
    "\n",
    "    files = {\n",
    "        \"random\": _pick(root, \"random\"),\n",
    "        \"popular\": _pick(root, \"popular\"),\n",
    "        \"adversarial\": _pick(root, \"adversarial\"),\n",
    "    }\n",
    "    print(\"Using files:\")\n",
    "    for k,p in files.items():\n",
    "        print(f\" - {k}: {p.relative_to(root)} (size {p.stat().st_size/1024:.1f} KB)\")\n",
    "\n",
    "    out = {}\n",
    "    for split, path in files.items():\n",
    "        raw_rows = _read_any(path)\n",
    "        norm = []\n",
    "        for r in raw_rows:\n",
    "            img = _canon_image(r)\n",
    "            q   = _canon_question(r)\n",
    "            a   = _canon_answer(r)\n",
    "            if img and q and a in {\"yes\",\"no\"}:\n",
    "                norm.append({\"image\": img, \"question\": q, \"answer\": a})\n",
    "        if not norm:\n",
    "            # show a few raw rows to help debug\n",
    "            print(f\"\\n[WARN] 0 normalized rows for {split}. First raw rows:\")\n",
    "            for sample in raw_rows[:3]:\n",
    "                print(sample)\n",
    "            raise RuntimeError(f\"0 normalized rows for split '{split}' from {path}\")\n",
    "        out[split] = norm\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053477f0-9b4a-43a6-914d-b1e0715338e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files:\n",
      " - random: coco_pope_random.json (size 362.5 KB)\n",
      " - popular: coco_pope_popular.json (size 361.5 KB)\n",
      " - adversarial: coco_pope_adversarial.json (size 361.7 KB)\n",
      "random: 3000 rows\n",
      "  sample: {'image': 'COCO_val2014_000000310196.jpg', 'question': 'Is there a snowboard in the image?', 'answer': 'yes'}\n",
      "popular: 3000 rows\n",
      "  sample: {'image': 'COCO_val2014_000000310196.jpg', 'question': 'Is there a snowboard in the image?', 'answer': 'yes'}\n",
      "adversarial: 3000 rows\n",
      "  sample: {'image': 'COCO_val2014_000000310196.jpg', 'question': 'Is there a snowboard in the image?', 'answer': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "pope = load_pope_split_robust(POPE_ROOT)\n",
    "\n",
    "for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "    print(f\"{split}: {len(pope[split])} rows\")\n",
    "    print(\"  sample:\", pope[split][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c2435b-0790-45d3-868b-dc939c368535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "oading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.51s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlavaForConditionalGeneration(\n",
       "  (model): LlavaModel(\n",
       "    (vision_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(577, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): LlavaMultiModalProjector(\n",
       "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (act): GELUActivation()\n",
       "      (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (language_model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: model + processor\n",
    "processor = AutoProcessor.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80136324-8649-4899-8f74-8286e015f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 decoder layers.\n"
     ]
    }
   ],
   "source": [
    "import contextlib, torch\n",
    "from PIL import Image\n",
    "\n",
    "def _get_llama_layers_from_llava(model):\n",
    "    \"\"\"\n",
    "    Return the list of decoder layers (LlamaDecoderLayer) robustly across HF LLaVA variants.\n",
    "    \"\"\"\n",
    "    # Common: LlavaForConditionalGeneration has .language_model as LlamaForCausalLM\n",
    "    if hasattr(model, \"language_model\"):\n",
    "        lm = model.language_model\n",
    "        # Case A: language_model is LlamaForCausalLM with .model (LlamaModel)\n",
    "        if hasattr(lm, \"model\") and hasattr(lm.model, \"layers\"):\n",
    "            return lm.model.layers\n",
    "        # Case B: language_model is already LlamaModel\n",
    "        if hasattr(lm, \"layers\"):\n",
    "            return lm.layers\n",
    "    # Fallbacks (rare)\n",
    "    if hasattr(model, \"model\") and hasattr(model.model, \"layers\"):\n",
    "        return model.model.layers\n",
    "    raise AttributeError(\"Could not locate LLaMA decoder layers in this HF LLaVA build.\")\n",
    "\n",
    "LAYERS = _get_llama_layers_from_llava(model)\n",
    "NUM_LAYERS = len(LAYERS)\n",
    "print(f\"Found {NUM_LAYERS} decoder layers.\")\n",
    "# Pick a high-ish layer (e.g., 28) safely\n",
    "STEER_LAYER_IDX = min(28, NUM_LAYERS - 1)  \n",
    "\n",
    "def _forward_with_layer_capture(inputs, layer_idx):\n",
    "    \"\"\"\n",
    "    Run a forward pass and capture the output of the chosen decoder layer.\n",
    "    Returns tensor of shape (1, hidden_size) for the **last token**.\n",
    "    \"\"\"\n",
    "    captured = {}\n",
    "    target_layer = LAYERS[layer_idx]\n",
    "\n",
    "    def hook_fn(module, inp, out):\n",
    "        # out: (batch, seq, hidden) -- we take the last token\n",
    "        captured[\"h\"] = out[:, -1, :].detach()\n",
    "        return out\n",
    "\n",
    "    with torch.no_grad(), contextlib.ExitStack() as stack:\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        # Use .__enter__() and .__exit__() for handle if contextlib.ExitStack is not available\n",
    "        stack.enter_context(handle)\n",
    "        _ = model(**inputs, output_hidden_states=False, use_cache=False)\n",
    "    return captured.get(\"h\")  # (1, hidden)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _layer_hidden_image(question, image, layer_idx):\n",
    "    prompt = (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    return _forward_with_layer_capture(inputs, layer_idx)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _layer_hidden_langonly(question, layer_idx):\n",
    "    # no <image> → pure language prior\n",
    "    prompt = (\n",
    "        \"USER:\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    return _forward_with_layer_capture(inputs, layer_idx)\n",
    "\n",
    "def build_midlayer_vector(pope, coco_img_root, layer_idx=STEER_LAYER_IDX, num_calib=600):\n",
    "    vecs = []\n",
    "    rows = pope[\"random\"][:num_calib]\n",
    "    for r in tqdm(rows, desc=f\"Build v @ layer {layer_idx}\", leave=False):\n",
    "        p = os.path.join(coco_img_root, r[\"image\"])\n",
    "        if not os.path.exists(p): \n",
    "            continue\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        h_img  = _layer_hidden_image(r[\"question\"], img, layer_idx)\n",
    "        h_lang = _layer_hidden_langonly(r[\"question\"], layer_idx)\n",
    "        if h_img is None or h_lang is None:\n",
    "            continue\n",
    "        vecs.append((h_img - h_lang).cpu())\n",
    "    if not vecs:\n",
    "        raise RuntimeError(\"No calibration vectors collected — check image paths/split names.\")\n",
    "    v = torch.stack(vecs).mean(0).squeeze(0)\n",
    "    v = v / (v.norm(p=2) + 1e-8)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09dee882-c846-46ec-b1f8-7ccebead7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid-layer v: (4096,) layer idx: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# --- Run this cell to build the vector ---\n",
    "steer_v_mid = build_midlayer_vector(pope, COCO_IMG_ROOT, layer_idx=STEER_LAYER_IDX, num_calib=600)\n",
    "steer_v_mid = steer_v_mid.to(model.device, dtype=model.dtype) # Ensure it's on the right device/dtype\n",
    "print(\"mid-layer v:\", tuple(steer_v_mid.shape), \"layer idx:\", STEER_LAYER_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7007746f-b8c7-443e-8894-d944b00c2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt + yes/no ids\n",
    "def yn_prompt(question: str) -> str:\n",
    "    return (\n",
    "        \"USER: <image>\\n\"\n",
    "        \"Answer the question strictly with 'Yes' or 'No'. Do not add any other words.\\n\"\n",
    "        f\"Question: {question}\\nASSISTANT:\"\n",
    "    )\n",
    "\n",
    "def get_yes_no_ids(tokenizer):\n",
    "    yes_ids = [tokenizer.convert_tokens_to_ids(\"Yes\"),\n",
    "               tokenizer.convert_tokens_to_ids(\"yes\")]\n",
    "    no_ids  = [tokenizer.convert_tokens_to_ids(\"No\"),\n",
    "               tokenizer.convert_tokens_to_ids(\"no\")]\n",
    "\n",
    "    yes_ids = [i for i in yes_ids if i is not None and i >= 0]\n",
    "    no_ids  = [i for i in no_ids  if i is not None and i >= 0]\n",
    "\n",
    "    # Fallback: first subtoken if convert_tokens_to_ids fails\n",
    "    if not yes_ids:\n",
    "        tid = tokenizer(\"Yes\", add_special_tokens=False).input_ids\n",
    "        if tid: yes_ids = [tid[0]]\n",
    "    if not no_ids:\n",
    "        tid = tokenizer(\"No\", add_special_tokens=False).input_ids\n",
    "        if tid: no_ids = [tid[0]]\n",
    "\n",
    "    if not yes_ids or not no_ids:\n",
    "        raise RuntimeError(\"Could not build YES/NO token id sets.\")\n",
    "\n",
    "    # dedupe\n",
    "    yes_ids = list(dict.fromkeys(yes_ids))\n",
    "    no_ids  = list(dict.fromkeys(no_ids))\n",
    "    return yes_ids, no_ids\n",
    "\n",
    "YES_IDS, NO_IDS = get_yes_no_ids(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb2662ff-bfb6-47b9-9bf9-2e59a300ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions (Metrics, eval_rows, pct, run_full_pope) are defined.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# --- From Cell 6 (Metrics + evaluator) ---\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    tp:int=0; tn:int=0; fp:int=0; fn:int=0\n",
    "    def scores(self):\n",
    "        acc = (self.tp + self.tn) / max(1, self.tp + self.tn + self.fp + self.fn)\n",
    "        prec = self.tp / max(1, self.tp + self.fp)\n",
    "        rec  = self.tp / max(1, self.tp + self.fn)\n",
    "        f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "        return {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}\n",
    "\n",
    "def eval_rows(rows: List[dict], coco_img_root: str, infer_fn) -> dict:\n",
    "    m = Metrics()\n",
    "    for r in tqdm(rows, leave=False, desc=\"Evaluating\"):\n",
    "        img_file = r[\"image\"]\n",
    "        img_path = img_file if os.path.isabs(img_file) else os.path.join(coco_img_root, img_file)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue  # skip missing files\n",
    "        \n",
    "        # Use the compatibility normalizer to ensure output is 'yes' or 'no'\n",
    "        pred = normalize_yesno_compat(infer_fn(img_path, r[\"question\"]))\n",
    "        gt   = r[\"answer\"]\n",
    "        \n",
    "        if   pred==\"yes\" and gt==\"yes\": m.tp += 1\n",
    "        elif pred==\"no\"  and gt==\"no\" : m.tn += 1\n",
    "        elif pred==\"yes\" and gt==\"no\" : m.fp += 1\n",
    "        elif pred==\"no\"  and gt==\"yes\": m.fn += 1\n",
    "    return m.scores()\n",
    "\n",
    "def pct(d):  # pretty-print as percentages\n",
    "    return {k: round(v*100, 2) for k,v in d.items()}\n",
    "\n",
    "# --- From Cell 8 (Full POPE run + report) ---\n",
    "\n",
    "def run_full_pope(pope, coco_img_root, infer_fn):\n",
    "    results = {}\n",
    "    counts  = {}\n",
    "    all_rows = []\n",
    "    t0 = time.time()\n",
    "    for split in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"\\nRunning split: {split}\")\n",
    "        s = eval_rows(pope[split], coco_img_root, infer_fn)\n",
    "        results[split] = s\n",
    "        all_rows.extend(pope[split])\n",
    "    \n",
    "    # Re-run eval on all rows combined for a true overall score\n",
    "    print(f\"\\nRunning split: Overall\")\n",
    "    overall = eval_rows(all_rows, coco_img_root, infer_fn)\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(\"\\nPOPE (COCO) results:\")\n",
    "    for k in (\"random\",\"popular\",\"adversarial\"):\n",
    "        print(f\"- {k.title():12}:\", pct(results[k]))\n",
    "    print(f\"- Overall     :\", pct(overall))\n",
    "    print(f\"\\nTotal time: {t1 - t0:.1f}s for {len(all_rows)} items\")\n",
    "    return results, overall\n",
    "\n",
    "# --- Add the 'normalize_yesno_compat' function ---\n",
    "# This ensures the output of infer_fn matches the evaluator's 'yes'/'no' strings\n",
    "def normalize_yesno_compat(s: str) -> str:\n",
    "    \"\"\"Return exactly 'yes' or 'no' to match eval_rows expectations.\"\"\"\n",
    "    t = (s or \"\").strip().lower()\n",
    "    # common variants\n",
    "    if t.startswith(\"y\"): return \"yes\"\n",
    "    if t.startswith(\"n\"): return \"no\"\n",
    "    # fallback: search tokens\n",
    "    if \"yes\" in t: return \"yes\"\n",
    "    if \"no\"  in t: return \"no\"\n",
    "    # absolute fallback: default to 'no'\n",
    "    return \"no\"\n",
    "\n",
    "print(\"Evaluation functions (Metrics, eval_rows, pct, run_full_pope) are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6de4dde6-c659-4359-9b3a-622ba41bccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def _make_contrast_image(\n",
    "    image: Image.Image,\n",
    "    mode: str = \"black\",\n",
    "    mask_ratio: float = 0.9,\n",
    "    seed: int | None = 1337,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a contrast image to *deflate* spurious cues for VCD.\n",
    "\n",
    "    Args:\n",
    "      image: PIL.Image (RGB recommended)\n",
    "      mode: one of {\"black\", \"gaussian\", \"random_mask\"}\n",
    "        - \"black\": return a solid-black image of same size\n",
    "        - \"gaussian\": return pure Gaussian-noise image (destroy content)\n",
    "        - \"random_mask\": randomly mask out `mask_ratio` of pixels to black\n",
    "      mask_ratio: for modes that use masking (0.0..1.0). 0.9 -> mask 90% pixels.\n",
    "      seed: optional RNG seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "      PIL.Image (RGB), same size as input\n",
    "    \"\"\"\n",
    "    assert 0.0 <= mask_ratio <= 1.0, \"mask_ratio must be in [0, 1]\"\n",
    "    img = image.convert(\"RGB\")\n",
    "    W, H = img.size\n",
    "\n",
    "    if mode == \"black\":\n",
    "        return Image.new(\"RGB\", (W, H), color=(0, 0, 0))\n",
    "\n",
    "    if seed is not None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "    else:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    if mode == \"gaussian\":\n",
    "        # Pure Gaussian noise image (mean 0.5, std 0.25, clipped to [0,1])\n",
    "        noise = rng.normal(loc=0.5, scale=0.25, size=(H, W, 3))\n",
    "        noise = np.clip(noise, 0.0, 1.0)\n",
    "        arr = (noise * 255).astype(np.uint8)\n",
    "        return Image.fromarray(arr, mode=\"RGB\")\n",
    "\n",
    "    if mode == \"random_mask\":\n",
    "        # Mask `mask_ratio` fraction of pixels to black; keep the rest unchanged.\n",
    "        arr = np.array(img, dtype=np.uint8)\n",
    "        # Bernoulli mask shape (H, W, 1): True => keep, False => mask to black\n",
    "        keep_prob = 1.0 - mask_ratio\n",
    "        keep = rng.random((H, W, 1)) < keep_prob\n",
    "        masked = arr.copy()\n",
    "        masked[~keep.repeat(3, axis=2)] = 0\n",
    "        return Image.fromarray(masked, mode=\"RGB\")\n",
    "\n",
    "    raise ValueError(f\"Unknown contrast mode: {mode!r}. \"\n",
    "                     \"Use one of: 'black', 'gaussian', 'random_mask'.\")\n",
    "    \n",
    "def _logits_last_token(image, prompt):\n",
    "    \"\"\"\n",
    "    Runs the model once on an <image> + prompt and returns the logits\n",
    "    at the final (last) token position.\n",
    "    \"\"\"\n",
    "    # Tokenize with image token inserted\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Forward pass\n",
    "    out = model(**inputs, output_hidden_states=False)\n",
    "    \n",
    "    # Extract logits of the last token\n",
    "    logits = out.logits[:, -1, :]  # shape: [1, vocab_size]\n",
    "    return logits\n",
    "\n",
    "    import torch\n",
    "from contextlib import ExitStack\n",
    "\n",
    "@torch.no_grad()\n",
    "def _asd_logits(\n",
    "    image,\n",
    "    prompt,\n",
    "    steer_layers,              # int or list[int] (0-indexed decoder block ids)\n",
    "    steer_v: torch.Tensor,     # shape: [hidden_size]\n",
    "    lam_pos: float = 0.2,      # kept for API compat; not used in this minimal ASD\n",
    "    lam_neg: float = 0.35,     # kept for API compat; not used in this minimal ASD\n",
    "    alpha: float   = 1.0,      # steering strength\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal ASD: add alpha * v at the chosen decoder layer(s), then get final-token logits.\n",
    "    - steer_layers: int or list of ints (e.g., 26, or [26,27,28])\n",
    "    - steer_v: steering vector (will be L2-normalized and broadcast over [B,T,H])\n",
    "    Returns: logits[:, -1, :]  (shape [1, vocab_size])\n",
    "    \"\"\"\n",
    "    # Normalize & move v\n",
    "    v = steer_v.to(model.device)\n",
    "    v = v / (v.norm(p=2) + 1e-9)  # L2-normalize\n",
    "    v = v.view(1, 1, -1)          # for broadcasting onto [B, T, H]\n",
    "\n",
    "    # Ensure list of layers\n",
    "    if isinstance(steer_layers, int):\n",
    "        layer_ids = [steer_layers]\n",
    "    else:\n",
    "        layer_ids = list(steer_layers)\n",
    "\n",
    "    # Distribute alpha across layers if multiple\n",
    "    per_layer_alpha = alpha / max(1, len(layer_ids))\n",
    "\n",
    "    # Build inputs\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ---- Hook that injects the steering vector on the residual stream ----\n",
    "    def _hook_add_v(_module, _inp, out):\n",
    "        # out: [B,T,H] or tuple(..., hidden_states)\n",
    "        h = out[0] if isinstance(out, tuple) else out\n",
    "        return h + per_layer_alpha * v\n",
    "\n",
    "    # Get decoder blocks; adjust this path if your model differs\n",
    "    try:\n",
    "        decoder_layers = model.model.layers\n",
    "    except AttributeError:\n",
    "        # Some variants use model.model.decoder.layers or model.model.transformer.layers\n",
    "        # If you hit this path, inspect(model) to locate your decoder blocks.\n",
    "        decoder_layers = model.model.decoder.layers\n",
    "\n",
    "    # Register hooks, run forward, remove hooks\n",
    "    with ExitStack() as stack:\n",
    "        handles = []\n",
    "        for L in layer_ids:\n",
    "            handles.append(decoder_layers[L].register_forward_hook(_hook_add_v))\n",
    "        out = model(**inputs, output_hidden_states=False)\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "    return out.logits[:, -1, :]  # final-token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc247a42-36eb-4a89-b946-f77de9fe7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_yesno_vcd_asd(\n",
    "    img_path, question,\n",
    "    steer_layers, steer_v,\n",
    "    lam_pos=0.2, lam_neg=0.35, alpha=1.0,    # ASD params\n",
    "    gamma=1.0, contrast_mode=\"black\", mask_ratio=0.9  # VCD params\n",
    "):\n",
    "    # Dependencies assumed: yn_prompt, _asd_logits, _logits_last_token, _make_contrast_image,\n",
    "    # YES_IDS, NO_IDS, processor/model on GPU.\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question)\n",
    "\n",
    "    # Good logits: ASD on original image\n",
    "    logits_main = _asd_logits(\n",
    "        image=img, prompt=prompt,\n",
    "        steer_layers=steer_layers, steer_v=steer_v,\n",
    "        lam_pos=lam_pos, lam_neg=lam_neg, alpha=alpha\n",
    "    )\n",
    "\n",
    "    # Bad logits: base model on contrast image (NO steering)\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio)\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt)\n",
    "\n",
    "    # VCD combine\n",
    "    logits = logits_main - gamma * logits_con\n",
    "\n",
    "    # Decode yes/no\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no  = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "\n",
    "try:\n",
    "    infer_yesno_vcd_asd_FIXED\n",
    "    infer_yesno_vcd_asd = infer_yesno_vcd_asd_FIXED\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def _normalize_yesno_compat(x: str) -> str:\n",
    "    x = (x or \"\").strip().lower()\n",
    "    if x in (\"y\", \"yes\", \"yeah\", \"yep\", \"true\"): return \"yes\"\n",
    "    if x in (\"n\", \"no\", \"nope\", \"false\"):         return \"no\"\n",
    "    # fall back to majority class \"no\" (POPE default is often balanced; pick one deterministically)\n",
    "    return \"no\"\n",
    "\n",
    "try:\n",
    "    normalize_yesno_compat\n",
    "except NameError:\n",
    "    normalize_yesno_compat = _normalize_yesno_compat\n",
    "\n",
    "\n",
    "try:\n",
    "    STEER_LAYERS = STEER_LAYER_IDX\n",
    "except NameError:\n",
    "    try:\n",
    "        STEER_LAYERS = [STEER_LAYER_IDX]  # single int -> list\n",
    "    except NameError:\n",
    "        STEER_LAYERS = [28]  # sane default for LLaVA/Vicuna-family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "966d0e09-643d-40cc-a7d1-0cac41402146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running smoke test (v2, Corrected Logic) on 300 samples...\n",
      "\n",
      "--- Smoke Test Results (v2, Corrected Logic, gamma=0.6) ---\n",
      "Overall (300 samples): {'acc': 74.0, 'precision': 67.14, 'recall': 94.0, 'f1': 78.33}\n",
      "\n",
      "Total time: 107.5s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_yesno_vcd_asd_FIXED(\n",
    "    img_path, question,\n",
    "    steer_layers, steer_v,\n",
    "    lam_pos=0.1, lam_neg=0.35, alpha=1.0,    # ASD params\n",
    "    gamma=0.6, contrast_mode=\"black\", mask_ratio=0.9 # VCD params\n",
    "):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question) # Uses yn_prompt from Cell 18\n",
    "    \n",
    "    # --- Good Logits (Main image + ASD steering) ---\n",
    "    logits_main = _asd_logits(\n",
    "        image=img, prompt=prompt, steer_layers=steer_layers, steer_v=steer_v,\n",
    "        lam_pos=lam_pos, lam_neg=lam_neg, alpha=alpha\n",
    "    )\n",
    "    \n",
    "    # --- Bad Logits (Contrast image, NO steering) ---\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio) # Uses _make_contrast_image from Cell 18\n",
    "    \n",
    "    # [THIS IS THE FIX]\n",
    "    # We use the base model's logits, not the ASD-steered logits\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt) # Uses _logits_last_token from Cell 18\n",
    "    \n",
    "    # --- Final VCD Logits ---\n",
    "    logits = logits_main - gamma * logits_con\n",
    "    \n",
    "    # Decode yes/no\n",
    "    probs  = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes  = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no   = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    \n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "\n",
    "# ===== 2) Define the inference function for the evaluator =====\n",
    "def infer_fn_smoke_test_v2(p, q):\n",
    "    return infer_yesno_vcd_asd_FIXED(\n",
    "        img_path=p, question=q,\n",
    "        steer_layers=STEER_LAYER_IDX, steer_v=steer_v_mid, # Assumes these are in memory\n",
    "        lam_pos=0.1, lam_neg=0.35, alpha=1.0,\n",
    "        gamma=0.6, contrast_mode=\"black\" # New gamma baseline\n",
    "    )\n",
    "\n",
    "# ===== 3) Create a 300-sample test set =====\n",
    "smoke_test_samples = (\n",
    "    pope[\"random\"][:100] + \n",
    "    pope[\"popular\"][:100] + \n",
    "    pope[\"adversarial\"][:100]\n",
    ")\n",
    "print(f\"Running smoke test (v2, Corrected Logic) on {len(smoke_test_samples)} samples...\")\n",
    "\n",
    "# ===== 4) Run the evaluation =====\n",
    "t0 = time.time()\n",
    "\n",
    "# Use the 'eval_rows' function defined earlier (in Cell 10 or Cell 18)\n",
    "smoke_results = eval_rows(smoke_test_samples, COCO_IMG_ROOT, infer_fn_smoke_test_v2)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"\\n--- Smoke Test Results (v2, Corrected Logic, gamma=0.6) ---\")\n",
    "pretty_results = {k: round(v * 100, 2) for k, v in smoke_results.items()}\n",
    "print(f\"Overall (300 samples): {pretty_results}\")\n",
    "print(f\"\\nTotal time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54c2ef21-9c4e-4f50-9226-97edae7d0ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running smoke test (v2, Corrected Logic) on 300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smoke Test Results (v2, Corrected Logic) ---\n",
      "Overall (300 samples): {'Accuracy': 84.33, 'Precision': 89.31, 'Recall': 78.0, 'F1': 83.27}\n",
      "\n",
      "Total time: 74.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm # Make sure tqdm is imported if not already\n",
    "\n",
    "# ===== 1) Define the CORRECTED inference function =====\n",
    "# This function contrasts ASD(img) with BaseModel(cimg)\n",
    "@torch.no_grad()\n",
    "def infer_yesno_vcd_asd_FIXED(\n",
    "    img_path, question,\n",
    "    steer_layers, steer_v,\n",
    "    lam_pos=0.2, lam_neg=0.35, alpha=4.0,    # ASD params\n",
    "    gamma=0, contrast_mode=\"black\", mask_ratio=0.9 # VCD params\n",
    "):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question) # Uses yn_prompt from Cell 18\n",
    "    \n",
    "    # --- Good Logits (Main image + ASD steering) ---\n",
    "    logits_main = _asd_logits(\n",
    "        image=img, prompt=prompt, steer_layers=steer_layers, steer_v=steer_v,\n",
    "        lam_pos=lam_pos, lam_neg=lam_neg, alpha=alpha\n",
    "    )\n",
    "    \n",
    "    # --- Bad Logits (Contrast image, NO steering) ---\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio) # Uses _make_contrast_image from Cell 18\n",
    "    \n",
    "    # [THIS IS THE FIX]\n",
    "    # We use the base model's logits, not the ASD-steered logits\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt) # Uses _logits_last_token from Cell 18\n",
    "    \n",
    "    # --- Final VCD Logits ---\n",
    "    logits = logits_main - gamma * logits_con\n",
    "    \n",
    "    # Decode yes/no\n",
    "    probs  = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes  = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no   = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    \n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "\n",
    "# ===== 2) Define the inference function for the evaluator =====\n",
    "def infer_fn_smoke_test_v2(p, q):\n",
    "    return infer_yesno_vcd_asd_FIXED(\n",
    "        img_path=p, question=q,\n",
    "        steer_layers=STEER_LAYER_IDX, steer_v=steer_v_mid, # Assumes these are in memory\n",
    "        lam_pos=0.2, lam_neg=0.35, alpha=1.0,\n",
    "        gamma=0, contrast_mode=\"black\" # New gamma baseline\n",
    "    )\n",
    "\n",
    "# ===== 3) Create a 300-sample test set =====\n",
    "smoke_test_samples = (\n",
    "    pope[\"random\"][:100] + \n",
    "    pope[\"popular\"][:100] + \n",
    "    pope[\"adversarial\"][:100]\n",
    ")\n",
    "print(f\"Running smoke test (v2, Corrected Logic) on {len(smoke_test_samples)} samples...\")\n",
    "\n",
    "# ===== 4) Run the evaluation =====\n",
    "t0 = time.time()\n",
    "\n",
    "# Use the 'eval_rows' function defined earlier (in Cell 10 or Cell 18)\n",
    "smoke_results = eval_rows(smoke_test_samples, COCO_IMG_ROOT, infer_fn_smoke_test_v2)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"\\n--- Smoke Test Results (v2, Corrected Logic) ---\")\n",
    "pretty_results = {k: round(v * 100, 2) for k, v in smoke_results.items()}\n",
    "print(f\"Overall (300 samples): {pretty_results}\")\n",
    "print(f\"\\nTotal time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e200d8e-87fd-4d7f-9387-b3e2ce46a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running VCD-only smoke test on 300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VCD-ONLY Smoke Test Results ---\n",
      "Overall (300 samples): {'Accuracy': 60.67, 'Precision': 55.97, 'Recall': 100.0, 'F1': 71.77}\n",
      "Total time: 72.0s\n",
      "\n",
      "[SWEEP] VCD gamma=0.5, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 82.33, 'Precision': 78.03, 'Recall': 90.0, 'F1': 83.59} | Time: 72.0s\n",
      "\n",
      "[SWEEP] VCD gamma=0.5, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 83.33, 'Precision': 82.89, 'Recall': 84.0, 'F1': 83.44} | Time: 73.0s\n",
      "\n",
      "[SWEEP] VCD gamma=1.0, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 60.67, 'Precision': 55.97, 'Recall': 100.0, 'F1': 71.77} | Time: 72.1s\n",
      "\n",
      "[SWEEP] VCD gamma=1.0, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 71.0, 'Precision': 66.67, 'Recall': 84.0, 'F1': 74.34} | Time: 71.9s\n",
      "\n",
      "[SWEEP] VCD gamma=2.0, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 50.0, 'Precision': 50.0, 'Recall': 100.0, 'F1': 66.67} | Time: 71.8s\n",
      "\n",
      "[SWEEP] VCD gamma=2.0, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 49.33, 'Precision': 49.66, 'Recall': 98.0, 'F1': 65.92} | Time: 72.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ===== VCD-ONLY SMOKE TEST =====\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm # Make sure tqdm is imported if not already\n",
    "\n",
    "# 1) Pure VCD yes/no inference (no ASD, no steering vectors at all)\n",
    "@torch.no_grad()\n",
    "def infer_yesno_vcd_only(\n",
    "    img_path: str,\n",
    "    question: str,\n",
    "    gamma: float = 1.0,            # VCD strength\n",
    "    contrast_mode: str = \"black\",  # e.g. \"black\", \"gaussian\", \"random_mask\"\n",
    "    mask_ratio: float = 0.9        # used by modes that support masking\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements Visual Contrastive Decoding for yes/no POPE prompts:\n",
    "      logits = logits(base image) - gamma * logits(contrast image)\n",
    "\n",
    "    Assumes availability of:\n",
    "      - yn_prompt(question)            -> str\n",
    "      - _logits_last_token(image,prompt) -> Tensor [1, vocab]\n",
    "      - _make_contrast_image(image, mode, mask_ratio) -> PIL.Image\n",
    "      - YES_IDS, NO_IDS (lists of token ids)\n",
    "    \"\"\"\n",
    "    # Prep prompt and images\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question)\n",
    "\n",
    "    # Base (faithful) logits\n",
    "    logits_base = _logits_last_token(image=img, prompt=prompt)   # [1, vocab]\n",
    "\n",
    "    # Contrast (unfaithful) logits\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio)\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt)   # [1, vocab]\n",
    "\n",
    "    # VCD combination\n",
    "    logits = logits_base - gamma * logits_con\n",
    "\n",
    "    # Decode Yes/No\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no  = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "\n",
    "# 2) Wrap for the evaluator\n",
    "def infer_fn_vcd_smoke(p, q):\n",
    "    return infer_yesno_vcd_only(\n",
    "        img_path=p,\n",
    "        question=q,\n",
    "        gamma=1.0,             # tweakable: try 0.5, 1.0, 2.0\n",
    "        contrast_mode=\"black\", # tweakable: \"black\", \"random_mask\", etc.\n",
    "        mask_ratio=0.9\n",
    "    )\n",
    "\n",
    "# 3) 300-sample smoke set (100 from each POPE split)\n",
    "smoke_test_samples = (\n",
    "    pope[\"random\"][:100] +\n",
    "    pope[\"popular\"][:100] +\n",
    "    pope[\"adversarial\"][:100]\n",
    ")\n",
    "print(f\"Running VCD-only smoke test on {len(smoke_test_samples)} samples...\")\n",
    "\n",
    "# 4) Evaluate\n",
    "t0 = time.time()\n",
    "smoke_results = eval_rows(smoke_test_samples, COCO_IMG_ROOT, infer_fn_vcd_smoke)\n",
    "t1 = time.time()\n",
    "\n",
    "pretty = {k: round(v * 100, 2) for k, v in smoke_results.items()}\n",
    "print(\"\\n--- VCD-ONLY Smoke Test Results ---\")\n",
    "print(f\"Overall (300 samples): {pretty}\")\n",
    "print(f\"Total time: {t1 - t0:.1f}s\")\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "gammas = [0.5, 1.0, 2.0]\n",
    "modes  = [\"black\", \"random_mask\"]   # add other modes your _make_contrast_image supports\n",
    "\n",
    "for g, m in product(gammas, modes):\n",
    "    def _infer(p, q, _g=g, _m=m):\n",
    "        return infer_yesno_vcd_only(p, q, gamma=_g, contrast_mode=_m, mask_ratio=0.9)\n",
    "    print(f\"\\n[SWEEP] VCD gamma={g}, mode='{m}'\")\n",
    "    t0 = time.time()\n",
    "    r = eval_rows(smoke_test_samples, COCO_IMG_ROOT, _infer)\n",
    "    t1 = time.time()\n",
    "    r_pct = {k: round(v * 100, 2) for k, v in r.items()}\n",
    "    print(f\"Overall: {r_pct} | Time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "799aefe0-e2fb-4b49-bf79-17f9deadb7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SWEEP] VCD gamma=0.2, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.67, 'Precision': 86.9, 'Recall': 84.0, 'F1': 85.42} | Time: 72.8s\n",
      "\n",
      "[SWEEP] VCD gamma=0.2, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.67, 'Precision': 88.49, 'Recall': 82.0, 'F1': 85.12} | Time: 72.7s\n",
      "\n",
      "[SWEEP] VCD gamma=0, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 84.33, 'Precision': 89.31, 'Recall': 78.0, 'F1': 83.27} | Time: 72.5s\n",
      "\n",
      "[SWEEP] VCD gamma=0, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 84.33, 'Precision': 89.31, 'Recall': 78.0, 'F1': 83.27} | Time: 73.0s\n",
      "\n",
      "[SWEEP] VCD gamma=-0.5, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 50.0, 'Precision': 50.0, 'Recall': 100.0, 'F1': 66.67} | Time: 73.5s\n",
      "\n",
      "[SWEEP] VCD gamma=-0.5, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 50.0, 'Precision': 50.0, 'Recall': 100.0, 'F1': 66.67} | Time: 72.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gammas = [0.2, 0, -0.5]\n",
    "modes  = [\"black\", \"random_mask\"]   # add other modes your _make_contrast_image supports\n",
    "\n",
    "for g, m in product(gammas, modes):\n",
    "    def _infer(p, q, _g=g, _m=m):\n",
    "        return infer_yesno_vcd_only(p, q, gamma=_g, contrast_mode=_m, mask_ratio=0.9)\n",
    "    print(f\"\\n[SWEEP] VCD gamma={g}, mode='{m}'\")\n",
    "    t0 = time.time()\n",
    "    r = eval_rows(smoke_test_samples, COCO_IMG_ROOT, _infer)\n",
    "    t1 = time.time()\n",
    "    r_pct = {k: round(v * 100, 2) for k, v in r.items()}\n",
    "    print(f\"Overall: {r_pct} | Time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d009e05-3b6b-401c-89de-0ad7af44582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SWEEP] VCD gamma=0.15, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.67, 'Precision': 86.9, 'Recall': 84.0, 'F1': 85.42} | Time: 72.2s\n",
      "\n",
      "[SWEEP] VCD gamma=0.15, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.33, 'Precision': 89.55, 'Recall': 80.0, 'F1': 84.51} | Time: 72.5s\n",
      "\n",
      "[SWEEP] VCD gamma=0.1, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.67, 'Precision': 88.49, 'Recall': 82.0, 'F1': 85.12} | Time: 74.7s\n",
      "\n",
      "[SWEEP] VCD gamma=0.1, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.33, 'Precision': 89.55, 'Recall': 80.0, 'F1': 84.51} | Time: 74.9s\n",
      "\n",
      "[SWEEP] VCD gamma=0.05, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.33, 'Precision': 89.55, 'Recall': 80.0, 'F1': 84.51} | Time: 76.2s\n",
      "\n",
      "[SWEEP] VCD gamma=0.05, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 84.33, 'Precision': 89.31, 'Recall': 78.0, 'F1': 83.27} | Time: 75.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gammas = [0.15, 0.1, 0.05]\n",
    "modes  = [\"black\", \"random_mask\"]   # add other modes your _make_contrast_image supports\n",
    "\n",
    "for g, m in product(gammas, modes):\n",
    "    def _infer(p, q, _g=g, _m=m):\n",
    "        return infer_yesno_vcd_only(p, q, gamma=_g, contrast_mode=_m, mask_ratio=0.9)\n",
    "    print(f\"\\n[SWEEP] VCD gamma={g}, mode='{m}'\")\n",
    "    t0 = time.time()\n",
    "    r = eval_rows(smoke_test_samples, COCO_IMG_ROOT, _infer)\n",
    "    t1 = time.time()\n",
    "    r_pct = {k: round(v * 100, 2) for k, v in r.items()}\n",
    "    print(f\"Overall: {r_pct} | Time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dde56ca-cfcc-4f6f-92d3-0a04b501cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SWEEP] VCD gamma=0.25, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 86.0, 'Precision': 86.0, 'Recall': 86.0, 'F1': 86.0} | Time: 72.3s\n",
      "\n",
      "[SWEEP] VCD gamma=0.25, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.67, 'Precision': 88.49, 'Recall': 82.0, 'F1': 85.12} | Time: 72.5s\n",
      "\n",
      "[SWEEP] VCD gamma=0.3, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 85.0, 'Precision': 84.31, 'Recall': 86.0, 'F1': 85.15} | Time: 72.6s\n",
      "\n",
      "[SWEEP] VCD gamma=0.3, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 84.67, 'Precision': 86.62, 'Recall': 82.0, 'F1': 84.25} | Time: 72.7s\n",
      "\n",
      "[SWEEP] VCD gamma=0.4, mode='black'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 84.0, 'Precision': 81.48, 'Recall': 88.0, 'F1': 84.62} | Time: 73.1s\n",
      "\n",
      "[SWEEP] VCD gamma=0.4, mode='random_mask'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall: {'Accuracy': 84.0, 'Precision': 84.0, 'Recall': 84.0, 'F1': 84.0} | Time: 74.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gammas = [0.25, 0.3, 0.4]\n",
    "modes  = [\"black\", \"random_mask\"]   # add other modes your _make_contrast_image supports\n",
    "\n",
    "for g, m in product(gammas, modes):\n",
    "    def _infer(p, q, _g=g, _m=m):\n",
    "        return infer_yesno_vcd_only(p, q, gamma=_g, contrast_mode=_m, mask_ratio=0.9)\n",
    "    print(f\"\\n[SWEEP] VCD gamma={g}, mode='{m}'\")\n",
    "    t0 = time.time()\n",
    "    r = eval_rows(smoke_test_samples, COCO_IMG_ROOT, _infer)\n",
    "    t1 = time.time()\n",
    "    r_pct = {k: round(v * 100, 2) for k, v in r.items()}\n",
    "    print(f\"Overall: {r_pct} | Time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02acc8f9-35ae-4216-b2b0-f1e2cee0d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from contextlib import ExitStack\n",
    "\n",
    "def _find_decoder_layers(_model):\n",
    "    \"\"\"\n",
    "    Return (layers_modulelist, qualified_name) for the *language decoder* layers.\n",
    "    We scan modules and pick the longest ModuleList named 'layers'.\n",
    "    \"\"\"\n",
    "    import torch.nn as nn\n",
    "    candidates = []\n",
    "    for name, mod in _model.named_modules():\n",
    "        if hasattr(mod, \"layers\") and isinstance(getattr(mod, \"layers\"), nn.ModuleList):\n",
    "            L = getattr(mod, \"layers\")\n",
    "            if len(L) >= 8:  # avoid tiny LayerNorm stacks etc.\n",
    "                candidates.append((name, L))\n",
    "    if not candidates:\n",
    "        raise AttributeError(\n",
    "            \"Could not find a '.layers' ModuleList inside the model. \"\n",
    "            \"Run: print([n for n,_ in model.named_modules() if n.endswith('layers')])\"\n",
    "        )\n",
    "    # choose the longest one (usually the language model decoder: length 32)\n",
    "    name, layers = max(candidates, key=lambda x: len(x[1]))\n",
    "    return layers, name\n",
    "\n",
    "# Cache the decoder layers once\n",
    "_DECODER_LAYERS, _DECODER_LAYERS_NAME = _find_decoder_layers(model)\n",
    "\n",
    "# 1) Minimal ASD logits (inject alpha * v at chosen layer(s))\n",
    "@torch.no_grad()\n",
    "def _asd_logits(\n",
    "    image,\n",
    "    prompt,\n",
    "    steer_layers,              # int or list[int], e.g., 28 or [26,27,28]\n",
    "    steer_v: torch.Tensor,     # shape [hidden_size]\n",
    "    lam_pos: float = 0.2,      # kept for API compat\n",
    "    lam_neg: float = 0.35,     # kept for API compat\n",
    "    alpha: float   = 1.0,\n",
    "):\n",
    "    # normalize v and shape to broadcast\n",
    "    v = steer_v.to(model.device)\n",
    "    v = v / (v.norm(p=2) + 1e-9)\n",
    "    v = v.view(1, 1, -1)\n",
    "\n",
    "    # ensure list of layer ids\n",
    "    if isinstance(steer_layers, int):\n",
    "        layer_ids = [steer_layers]\n",
    "    else:\n",
    "        layer_ids = list(steer_layers)\n",
    "\n",
    "    per_layer_alpha = alpha / max(1, len(layer_ids))\n",
    "\n",
    "    # inputs\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    def _hook_add_v(_module, _inp, out):\n",
    "        h = out[0] if isinstance(out, tuple) else out\n",
    "        return h + per_layer_alpha * v\n",
    "\n",
    "    # register hooks on the located decoder layers\n",
    "    with ExitStack() as stack:\n",
    "        handles = []\n",
    "        for L in layer_ids:\n",
    "            handles.append(_DECODER_LAYERS[L].register_forward_hook(_hook_add_v))\n",
    "        out = model(**inputs, output_hidden_states=False)\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "    return out.logits[:, -1, :]  # [1, vocab]\n",
    "\n",
    "# 2) VCD+ASD inference with your exact function name\n",
    "@torch.no_grad()\n",
    "def infer_yesno_vcd_asd(\n",
    "    img_path, question,\n",
    "    steer_layers, steer_v,\n",
    "    lam_pos=0.2, lam_neg=0.35, alpha=1.0,     # ASD\n",
    "    gamma=1.0, contrast_mode=\"black\", mask_ratio=0.9  # VCD\n",
    "):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question)\n",
    "\n",
    "    # Good path: ASD on the original\n",
    "    logits_main = _asd_logits(\n",
    "        image=img, prompt=prompt,\n",
    "        steer_layers=steer_layers, steer_v=steer_v,\n",
    "        lam_pos=lam_pos, lam_neg=lam_neg, alpha=alpha\n",
    "    )\n",
    "\n",
    "    # Contrast path: base (UNSTEERED) on contrast image\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio)\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt)\n",
    "\n",
    "    # VCD combine\n",
    "    logits = logits_main - gamma * logits_con\n",
    "\n",
    "    # Decode yes/no with your existing IDs\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no  = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "# 3) A smoke wrapper that uses your exact variable names (STEER_LAYER_IDX, steer_v_mid)\n",
    "def infer_fn_smoke_test(p, q):\n",
    "    return infer_yesno_vcd_asd(\n",
    "        img_path=p, question=q,\n",
    "        steer_layers=STEER_LAYER_IDX, steer_v=steer_v_mid,\n",
    "        lam_pos=0.2, lam_neg=0.35, alpha=1.0,\n",
    "        gamma=0.9, contrast_mode=\"black\", mask_ratio=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca6169b1-12f6-45bf-9ea2-84c8645e41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing POPE rows...\n",
      "Starting full POPE eval with ASD+VCD on 9000 items...\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random => {'Accuracy': 88.87, 'Precision': 95.69, 'Recall': 81.4, 'F1': 87.97}\n",
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular => {'Accuracy': 86.83, 'Precision': 91.32, 'Recall': 81.4, 'F1': 86.08}\n",
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial => {'Accuracy': 83.8, 'Precision': 85.5, 'Recall': 81.4, 'F1': 83.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ASD + VCD (Fixed) | FULL 3000 RESULTS ===\n",
      "      Random: {'Accuracy': 88.87, 'Precision': 95.69, 'Recall': 81.4, 'F1': 87.97}\n",
      "     Popular: {'Accuracy': 86.83, 'Precision': 91.32, 'Recall': 81.4, 'F1': 86.08}\n",
      " Adversarial: {'Accuracy': 83.8, 'Precision': 85.5, 'Recall': 81.4, 'F1': 83.4}\n",
      "     Overall: {'Accuracy': 86.5, 'Precision': 90.65, 'Recall': 81.4, 'F1': 85.77}\n",
      "Total time: 4360.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ===== FULL POPE (3000) EVAL: ASD + VCD =====\n",
    "import time, csv, os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# Config (edit as needed)\n",
    "# -------------------------\n",
    "STEER_LAYERS = STEER_LAYER_IDX   # e.g., [26, 27, 28] or a single int like 27\n",
    "STEER_V      = steer_v_mid        # your mid-layer steering vector (Tensor)\n",
    "\n",
    "# ASD params (typical good starting points)\n",
    "LAM_POS = 0.20\n",
    "LAM_NEG = 0.35\n",
    "ALPHA   = 4.0\n",
    "\n",
    "# VCD params\n",
    "GAMMA          = 0.2             # try 0.5–2.0\n",
    "CONTRAST_MODE  = \"black\"          # \"black\", \"gaussian\", or \"random_mask\"\n",
    "MASK_RATIO     = 0.90             # used when mode supports masking\n",
    "\n",
    "# -------------------------\n",
    "# Inference: ASD + VCD (Fixed)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def infer_yesno_asd_plus_vcd(\n",
    "    img_path: str,\n",
    "    question: str,\n",
    "    steer_layers,\n",
    "    steer_v: torch.Tensor,\n",
    "    lam_pos: float = 0.2,\n",
    "    lam_neg: float = 0.35,\n",
    "    alpha: float   = 1.0,\n",
    "    gamma: float   = 1.0,\n",
    "    contrast_mode: str = \"black\",\n",
    "    mask_ratio: float  = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Final logits = ASD(image) - gamma * Base(contrast_image).\n",
    "    Contrast branch is intentionally unsteered.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question)\n",
    "\n",
    "    # Good logits: ASD-steered on the original image\n",
    "    logits_main = _asd_logits(\n",
    "        image=img, prompt=prompt,\n",
    "        steer_layers=steer_layers, steer_v=steer_v,\n",
    "        lam_pos=lam_pos, lam_neg=lam_neg, alpha=alpha\n",
    "    )  # Tensor [1, vocab]\n",
    "\n",
    "    # Bad logits: base model on contrast image (NO steering)\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio)\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt)  # Tensor [1, vocab]\n",
    "\n",
    "    # VCD combine\n",
    "    logits = logits_main - gamma * logits_con\n",
    "\n",
    "    # Decode Yes/No\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no  = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Evaluator wrapper\n",
    "# -------------------------\n",
    "def infer_fn_asd_vcd(p, q):\n",
    "    return infer_yesno_asd_plus_vcd(\n",
    "        img_path=p, question=q,\n",
    "        steer_layers=STEER_LAYERS, steer_v=STEER_V,\n",
    "        lam_pos=LAM_POS, lam_neg=LAM_NEG, alpha=ALPHA,\n",
    "        gamma=GAMMA, contrast_mode=CONTRAST_MODE, mask_ratio=MASK_RATIO\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Full run on all 3 splits\n",
    "# -------------------------\n",
    "all_rows = []\n",
    "print(\"Preparing POPE rows...\")\n",
    "for split in (\"random\", \"popular\", \"adversarial\"):\n",
    "    all_rows.extend(pope[split])\n",
    "\n",
    "print(f\"Starting full POPE eval with ASD+VCD on {len(all_rows)} items...\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Evaluate per-split for clarity\n",
    "results = {}\n",
    "for split in (\"random\", \"popular\", \"adversarial\"):\n",
    "    print(f\"\\nRunning split: {split}\")\n",
    "    s = eval_rows(pope[split], COCO_IMG_ROOT, infer_fn_asd_vcd)\n",
    "    results[split] = {k: round(v * 100, 2) for k, v in s.items()}\n",
    "    print(f\"{split} => {results[split]}\")\n",
    "\n",
    "# Overall\n",
    "overall = eval_rows(all_rows, COCO_IMG_ROOT, infer_fn_asd_vcd)\n",
    "overall_pct = {k: round(v * 100, 2) for k, v in overall.items()}\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"\\n=== ASD + VCD (Fixed) | FULL 3000 RESULTS ===\")\n",
    "for split in (\"random\", \"popular\", \"adversarial\"):\n",
    "    print(f\"{split.capitalize():>12}: {results[split]}\")\n",
    "print(f\"{'Overall':>12}: {overall_pct}\")\n",
    "print(f\"Total time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac591df0-54dc-4c67-9906-397ff3d930da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing POPE rows...\n",
      "Starting full POPE eval with ASD+VCD on 9000 items...\n",
      "\n",
      "Running split: random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random => {'Accuracy': 89.73, 'Precision': 95.02, 'Recall': 83.87, 'F1': 89.09}\n",
      "\n",
      "Running split: popular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular => {'Accuracy': 87.07, 'Precision': 89.6, 'Recall': 83.87, 'F1': 86.64}\n",
      "\n",
      "Running split: adversarial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial => {'Accuracy': 83.17, 'Precision': 82.71, 'Recall': 83.87, 'F1': 83.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ASD + VCD (Fixed) | FULL 3000 RESULTS ===\n",
      "      Random: {'Accuracy': 89.73, 'Precision': 95.02, 'Recall': 83.87, 'F1': 89.09}\n",
      "     Popular: {'Accuracy': 87.07, 'Precision': 89.6, 'Recall': 83.87, 'F1': 86.64}\n",
      " Adversarial: {'Accuracy': 83.17, 'Precision': 82.71, 'Recall': 83.87, 'F1': 83.28}\n",
      "     Overall: {'Accuracy': 86.66, 'Precision': 88.82, 'Recall': 83.87, 'F1': 86.27}\n",
      "Total time: 4462.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ===== FULL POPE (3000) EVAL: ASD + VCD =====\n",
    "import time, csv, os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# Config (edit as needed)\n",
    "# -------------------------\n",
    "STEER_LAYERS = STEER_LAYER_IDX   # e.g., [26, 27, 28] or a single int like 27\n",
    "STEER_V      = steer_v_mid        # your mid-layer steering vector (Tensor)\n",
    "\n",
    "# ASD params (typical good starting points)\n",
    "LAM_POS = 0.20\n",
    "LAM_NEG = 0.35\n",
    "ALPHA   = 1.0\n",
    "\n",
    "# VCD params\n",
    "GAMMA          = 0.25             # try 0.5–2.0\n",
    "CONTRAST_MODE  = \"black\"          # \"black\", \"gaussian\", or \"random_mask\"\n",
    "MASK_RATIO     = 0.90             # used when mode supports masking\n",
    "\n",
    "# -------------------------\n",
    "# Inference: ASD + VCD (Fixed)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def infer_yesno_asd_plus_vcd(\n",
    "    img_path: str,\n",
    "    question: str,\n",
    "    steer_layers,\n",
    "    steer_v: torch.Tensor,\n",
    "    lam_pos: float = 0.2,\n",
    "    lam_neg: float = 0.35,\n",
    "    alpha: float   = 1.0,\n",
    "    gamma: float   = 1.0,\n",
    "    contrast_mode: str = \"black\",\n",
    "    mask_ratio: float  = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Final logits = ASD(image) - gamma * Base(contrast_image).\n",
    "    Contrast branch is intentionally unsteered.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question)\n",
    "\n",
    "    # Good logits: ASD-steered on the original image\n",
    "    logits_main = _asd_logits(\n",
    "        image=img, prompt=prompt,\n",
    "        steer_layers=steer_layers, steer_v=steer_v,\n",
    "        lam_pos=lam_pos, lam_neg=lam_neg, alpha=alpha\n",
    "    )  # Tensor [1, vocab]\n",
    "\n",
    "    # Bad logits: base model on contrast image (NO steering)\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio)\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt)  # Tensor [1, vocab]\n",
    "\n",
    "    # VCD combine\n",
    "    logits = logits_main - gamma * logits_con\n",
    "\n",
    "    # Decode Yes/No\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no  = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Evaluator wrapper\n",
    "# -------------------------\n",
    "def infer_fn_asd_vcd(p, q):\n",
    "    return infer_yesno_asd_plus_vcd(\n",
    "        img_path=p, question=q,\n",
    "        steer_layers=STEER_LAYERS, steer_v=STEER_V,\n",
    "        lam_pos=LAM_POS, lam_neg=LAM_NEG, alpha=ALPHA,\n",
    "        gamma=GAMMA, contrast_mode=CONTRAST_MODE, mask_ratio=MASK_RATIO\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Full run on all 3 splits\n",
    "# -------------------------\n",
    "all_rows = []\n",
    "print(\"Preparing POPE rows...\")\n",
    "for split in (\"random\", \"popular\", \"adversarial\"):\n",
    "    all_rows.extend(pope[split])\n",
    "\n",
    "print(f\"Starting full POPE eval with ASD+VCD on {len(all_rows)} items...\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Evaluate per-split for clarity\n",
    "results = {}\n",
    "for split in (\"random\", \"popular\", \"adversarial\"):\n",
    "    print(f\"\\nRunning split: {split}\")\n",
    "    s = eval_rows(pope[split], COCO_IMG_ROOT, infer_fn_asd_vcd)\n",
    "    results[split] = {k: round(v * 100, 2) for k, v in s.items()}\n",
    "    print(f\"{split} => {results[split]}\")\n",
    "\n",
    "# Overall\n",
    "overall = eval_rows(all_rows, COCO_IMG_ROOT, infer_fn_asd_vcd)\n",
    "overall_pct = {k: round(v * 100, 2) for k, v in overall.items()}\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"\\n=== ASD + VCD (Fixed) | FULL 3000 RESULTS ===\")\n",
    "for split in (\"random\", \"popular\", \"adversarial\"):\n",
    "    print(f\"{split.capitalize():>12}: {results[split]}\")\n",
    "print(f\"{'Overall':>12}: {overall_pct}\")\n",
    "print(f\"Total time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbf7e2b1-7146-4287-87af-a8324d24ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running smoke test (v2, Corrected Logic) on 300 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smoke Test Results (v2, Corrected Logic) ---\n",
      "Overall (300 samples): {'Accuracy': 86.67, 'Precision': 87.16, 'Recall': 86.0, 'F1': 86.58}\n",
      "\n",
      "Total time: 72.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm # Make sure tqdm is imported if not already\n",
    "\n",
    "# ===== 1) Define the CORRECTED inference function =====\n",
    "# This function contrasts ASD(img) with BaseModel(cimg)\n",
    "@torch.no_grad()\n",
    "def infer_yesno_vcd_asd_FIXED(\n",
    "    img_path, question,\n",
    "    steer_layers, steer_v,\n",
    "    lam_pos=0.2, lam_neg=0.35, alpha=1.0,    # ASD params\n",
    "    gamma=0, contrast_mode=\"black\", mask_ratio=0.9 # VCD params\n",
    "):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    prompt = yn_prompt(question) # Uses yn_prompt from Cell 18\n",
    "    \n",
    "    # --- Good Logits (Main image + ASD steering) ---\n",
    "    logits_main = _asd_logits(\n",
    "        image=img, prompt=prompt, steer_layers=steer_layers, steer_v=steer_v,\n",
    "        lam_pos=lam_pos, lam_neg=lam_neg, alpha=alpha\n",
    "    )\n",
    "    \n",
    "    # --- Bad Logits (Contrast image, NO steering) ---\n",
    "    cimg = _make_contrast_image(img, mode=contrast_mode, mask_ratio=mask_ratio) # Uses _make_contrast_image from Cell 18\n",
    "    \n",
    "    # [THIS IS THE FIX]\n",
    "    # We use the base model's logits, not the ASD-steered logits\n",
    "    logits_con = _logits_last_token(image=cimg, prompt=prompt) # Uses _logits_last_token from Cell 18\n",
    "    \n",
    "    # --- Final VCD Logits ---\n",
    "    logits = logits_main - gamma * logits_con\n",
    "    \n",
    "    # Decode yes/no\n",
    "    probs  = torch.softmax(logits, dim=-1)[0]\n",
    "    p_yes  = probs[YES_IDS].sum() if YES_IDS else torch.tensor(0.0, device=probs.device)\n",
    "    p_no   = probs[NO_IDS].sum()  if NO_IDS  else torch.tensor(0.0, device=probs.device)\n",
    "    \n",
    "    return \"yes\" if float(p_yes) >= float(p_no) else \"no\"\n",
    "\n",
    "\n",
    "# ===== 2) Define the inference function for the evaluator =====\n",
    "def infer_fn_smoke_test_v2(p, q):\n",
    "    return infer_yesno_vcd_asd_FIXED(\n",
    "        img_path=p, question=q,\n",
    "        steer_layers=STEER_LAYER_IDX, steer_v=steer_v_mid, # Assumes these are in memory\n",
    "        lam_pos=0.2, lam_neg=0.35, alpha=0.5,\n",
    "        gamma=0.25, contrast_mode=\"black\" # New gamma baseline\n",
    "    )\n",
    "\n",
    "# ===== 3) Create a 300-sample test set =====\n",
    "smoke_test_samples = (\n",
    "    pope[\"random\"][:100] + \n",
    "    pope[\"popular\"][:100] + \n",
    "    pope[\"adversarial\"][:100]\n",
    ")\n",
    "print(f\"Running smoke test (v2, Corrected Logic) on {len(smoke_test_samples)} samples...\")\n",
    "\n",
    "# ===== 4) Run the evaluation =====\n",
    "t0 = time.time()\n",
    "\n",
    "# Use the 'eval_rows' function defined earlier (in Cell 10 or Cell 18)\n",
    "smoke_results = eval_rows(smoke_test_samples, COCO_IMG_ROOT, infer_fn_smoke_test_v2)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"\\n--- Smoke Test Results (v2, Corrected Logic) ---\")\n",
    "pretty_results = {k: round(v * 100, 2) for k, v in smoke_results.items()}\n",
    "print(f\"Overall (300 samples): {pretty_results}\")\n",
    "print(f\"\\nTotal time: {t1 - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268077c-9e4c-4d8a-ac8b-1aaa695b287f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llava)",
   "language": "python",
   "name": "llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
